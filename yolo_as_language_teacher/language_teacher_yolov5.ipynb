{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_as_language_teacher.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP1TuA6X+ou9ufRrvD156ft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/Medium/blob/main/yolo_as_language_teacher/language_teacher_yolov5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3IR2E-4bPaO"
      },
      "source": [
        "# Object Detection & Language Learning\n",
        "Learning language is a great challenge. It takes a long time to acquire a foreign language. Though there free language tutorials and Apps that make it easier and enjoyable, language learning is still hard. With the advancement of Deep Learning, why can't we use it help us learning a new language?\n",
        "\n",
        "Let's start with an object detection AI which can be used instead of flash cards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmr3ahqma9uk"
      },
      "source": [
        "# Setup\n",
        "Install requirements and prepare the dataset for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fNDvNWOWvpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4c98e5-1687-4e5f-8a2a-8bf7a1ea2f19"
      },
      "source": [
        "!pip install pafy\n",
        "!pip install -q youtube-dl\n",
        "\n",
        "!pip install yolov5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pafy in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: yolov5 in /usr/local/lib/python3.7/dist-packages (6.1.2)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.11.0+cu113)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (3.2.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.5.5.64)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.4.0)\n",
            "Requirement already satisfied: boto3>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.23.5)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.64.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.4.1)\n",
            "Requirement already satisfied: sahi>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.9.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (6.0)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (9.1.1)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.5 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (1.26.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (1.25.11)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.2->yolov5) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->yolov5) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2021.10.8)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (3.1.10)\n",
            "Requirement already satisfied: click==8.0.4 in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (8.0.4)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (1.8.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click==8.0.4->sahi>=0.9.1->yolov5) (4.11.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.46.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click==8.0.4->sahi>=0.9.1->yolov5) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (3.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->yolov5) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1sEH6MGFlR"
      },
      "source": [
        "git clone Medium and yolov5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzHMg6kNuJay",
        "outputId": "387325c4-7629-43d2-ade5-b8dc51298d5d"
      },
      "source": [
        "!git clone https://github.com/changsin/Medium/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Medium'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 55 (delta 16), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/changsin/DLTrafficCounter/"
      ],
      "metadata": {
        "id": "Z2YIoCzRm_TR",
        "outputId": "32e5dca7-869a-40c7-c400-c8aa8c08ea15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLTrafficCounter'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 277 (delta 17), reused 66 (delta 15), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (277/277), 226.60 MiB | 27.45 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Checking out files: 100% (205/205), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sp8S964yARl",
        "outputId": "f70c8016-92b3-47c2-ae06-9d453732a6d8"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.11.0+cu113 (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccpQvXLlatKP"
      },
      "source": [
        "Download pretrained yolov5 model\n",
        "Choose one of the pretrained models from https://github.com/ultralytics/yolov5#inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5pTcJ1fyG5T",
        "outputId": "2543fe2b-3ef5-4d1a-8b9b-4a923d19d27e"
      },
      "source": [
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-21 03:13:16--  https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220521T031316Z&X-Amz-Expires=300&X-Amz-Signature=9f580d3c13170ef5235f19476bb3b96e939eb6652128210a797a3a4d1fbeeda8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-05-21 03:13:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220521T031316Z&X-Amz-Expires=300&X-Amz-Signature=9f580d3c13170ef5235f19476bb3b96e939eb6652128210a797a3a4d1fbeeda8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14795158 (14M) [application/octet-stream]\n",
            "Saving to: ‘yolov5s.pt’\n",
            "\n",
            "yolov5s.pt          100%[===================>]  14.11M  40.4MB/s    in 0.3s    \n",
            "\n",
            "2022-05-21 03:13:17 (40.4 MB/s) - ‘yolov5s.pt’ saved [14795158/14795158]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1pTl7FuAVc"
      },
      "source": [
        "# YOLOv5 Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOm6wEw-rxG-",
        "outputId": "914142b7-293f-4595-9a21-a1a78913a93c"
      },
      "source": [
        "%cd yolov5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'yolov5'\n",
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlIHqa6lJEgy"
      },
      "source": [
        "# Detect and compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g--Y7rPweNrM"
      },
      "source": [
        "## Pretrained model (baseline)\n",
        "- YOLOV5 Default Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.5 --source ../DLTrafficCounter/data/bbox_highway/test --data ../Medium/yolo_as_language_teacher/coco128-vi.yaml"
      ],
      "metadata": {
        "id": "A-r6imfTRUiy",
        "outputId": "ea947e02-3add-46b8-e3d2-42b1a27cbb50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=../DLTrafficCounter/data/bbox_highway/test, data=../Medium/yolo_as_language_teacher/coco128-vi.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-207-g5774a15 Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 224 layers, 7266973 parameters, 0 gradients\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 252, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 247, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 103, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 187, in __init__\n",
            "    raise Exception(f'ERROR: {p} does not exist')\n",
            "Exception: ERROR: /content/DLTrafficCounter/data/bbox_highway/test does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePHBhTXUJbRZ"
      },
      "source": [
        "- Run against the customized and better vehicle detection model.\n",
        "(If you are running yourself, you need to modify the path of the weights file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CML-7VNysJSG"
      },
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source ../DLTrafficCounter/data/bbox_highway/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeGM4abIWYZ8",
        "outputId": "0c19564f-4f2f-423c-df1e-1748bfa13635"
      },
      "source": [
        "!python detect.py --weights ../DLTrafficCounter/models/yolov5s_highway.pt --img 640 --conf 0.5 --source ../DLTrafficCounter/data/bbox_highway/test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['../DLTrafficCounter/models/yolov5s_highway.pt'], source=../DLTrafficCounter/data/bbox_highway/test, imgsz=640, conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 🚀 v5.0-351-ge96c74b torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n",
            "image 1/5 /content/yolov5/yolov5/../DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_096.png: 384x640 12 cars, 5 trucks, Done. (0.009s)\n",
            "image 2/5 /content/yolov5/yolov5/../DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_097.png: 384x640 8 cars, 7 trucks, Done. (0.007s)\n",
            "image 3/5 /content/yolov5/yolov5/../DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_098.png: 384x640 38 cars, 22 trucks, Done. (0.007s)\n",
            "image 4/5 /content/yolov5/yolov5/../DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_099.png: 384x640 47 cars, 3 buss, 17 trucks, Done. (0.007s)\n",
            "image 5/5 /content/yolov5/yolov5/../DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_100.png: 384x640 38 cars, 1 bus, 17 trucks, Done. (0.007s)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n",
            "Done. (0.785s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MswmiNkiR_ON"
      },
      "source": [
        "# Detect and Count\n",
        "To detect and count each vehicle type, we need to parse the detection results returned by YOLO. Here is a brief explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL5gKdpA_VCg"
      },
      "source": [
        "## Explanation of detection results\n",
        "\n",
        "1. results.names contain the names of classes: e.g., 'person'. There are 80 of them by default corresponding to 80 COCO dataset classes.\n",
        "\n",
        "  ['person',\n",
        " 'bicycle',\n",
        " 'car',\n",
        " 'motorcycle',\n",
        " 'airplane',\n",
        " 'bus',\n",
        " 'train',\n",
        " 'truck',\n",
        " 'boat',\n",
        " 'traffic light',\n",
        " 'fire hydrant',\n",
        " 'stop sign',\n",
        " 'parking meter',\n",
        " 'bench',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'dog',\n",
        " 'horse',\n",
        " 'sheep',\n",
        " 'cow',\n",
        " 'elephant',\n",
        " 'bear',\n",
        " 'zebra',\n",
        " 'giraffe',\n",
        " 'backpack',\n",
        " 'umbrella',\n",
        " 'handbag',\n",
        " 'tie',\n",
        " 'suitcase',\n",
        " 'frisbee',\n",
        " 'skis',\n",
        " 'snowboard',\n",
        " 'sports ball',\n",
        " 'kite',\n",
        " 'baseball bat',\n",
        " 'baseball glove',\n",
        " 'skateboard',\n",
        " 'surfboard',\n",
        " 'tennis racket',\n",
        " 'bottle',\n",
        " 'wine glass',\n",
        " 'cup',\n",
        " 'fork',\n",
        " 'knife',\n",
        " 'spoon',\n",
        " 'bowl',\n",
        " 'banana',\n",
        " 'apple',\n",
        " 'sandwich',\n",
        " 'orange',\n",
        " 'broccoli',\n",
        " 'carrot',\n",
        " 'hot dog',\n",
        " 'pizza',\n",
        " 'donut',\n",
        " 'cake',\n",
        " 'chair',\n",
        " 'couch',\n",
        " 'potted plant',\n",
        " 'bed',\n",
        " 'dining table',\n",
        " 'toilet',\n",
        " 'tv',\n",
        " 'laptop',\n",
        " 'mouse',\n",
        " 'remote',\n",
        " 'keyboard',\n",
        " 'cell phone',\n",
        " 'microwave',\n",
        " 'oven',\n",
        " 'toaster',\n",
        " 'sink',\n",
        " 'refrigerator',\n",
        " 'book',\n",
        " 'clock',\n",
        " 'vase',\n",
        " 'scissors',\n",
        " 'teddy bear',\n",
        " 'hair drier',\n",
        " 'toothbrush']\n",
        " \n",
        "\n",
        "2. results.xyxyn: xy coordinates followed by the confidence and the class id. For instance, the first item is class_id=0 with 90% confidence which refers to 'person' class\n",
        "\n",
        "```\n",
        "[tensor([[ 0.73203,  0.43620,  0.85469,  0.88646,  0.90088,  0.00000],\n",
        "         [ 0.70586,  0.36276,  0.92344,  0.49609,  0.62939, 25.00000],\n",
        "         [ 0.58125,  0.40365,  0.73984,  0.78594,  0.46143, 77.00000],\n",
        "         [ 0.39355,  0.15990,  0.58789,  0.80365,  0.44385, 10.00000],\n",
        "         [ 0.19248,  0.50104,  0.20469,  0.54062,  0.29517,  0.00000]], device='cuda:0')]\n",
        "```\n",
        "results.xyxy and results.pred have the same content except in scientific notations.\n",
        "```\n",
        "[tensor([[1.75687e+03, 7.85156e+02, 2.05125e+03, 1.59562e+03, 9.00879e-01, 0.00000e+00],\n",
        "         [1.69406e+03, 6.52969e+02, 2.21625e+03, 8.92969e+02, 6.29395e-01, 2.50000e+01],\n",
        "         [1.39500e+03, 7.26562e+02, 1.77562e+03, 1.41469e+03, 4.61426e-01, 7.70000e+01],\n",
        "         [9.44531e+02, 2.87812e+02, 1.41094e+03, 1.44656e+03, 4.43848e-01, 1.00000e+01],\n",
        "         [4.61953e+02, 9.01875e+02, 4.91250e+02, 9.73125e+02, 2.95166e-01, 0.00000e+00]], device='cuda:0')]\n",
        "```\n",
        "3. results.imgs is the labeled image containing the detection results.\n",
        "4. results.save('folder') saves the detection result image to the folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t5lmfBT_6jl"
      },
      "source": [
        "With this information, we can now parse and count each vehicle type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8xTskHP5jAy"
      },
      "source": [
        "## Plot annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJnZzGK2bh6F"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "def glob_files(path, file_type=\"*\"):\n",
        "    search_string = os.path.join(path, file_type)\n",
        "    files = glob.glob(search_string)\n",
        "\n",
        "    # print('searching ', path)\n",
        "    paths = []\n",
        "    for f in files:\n",
        "      if os.path.isdir(f):\n",
        "        sub_paths = glob_files(f + '/')\n",
        "        paths += sub_paths\n",
        "      else:\n",
        "        paths.append(f)\n",
        "\n",
        "    # We sort the images in alphabetical order to match them\n",
        "    #  to the annotation files\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ_C0gh4TdbZ"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "IMAGE_SIZE = 600\n",
        "\n",
        "def load_images(path):\n",
        "  files = glob_files(path, \"*.png\")\n",
        "\n",
        "  # print(files)\n",
        "  X_data = []\n",
        "  for file in files:\n",
        "    image = cv2.imread(file)\n",
        "    # print(image.shape)\n",
        "    # x = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    X_data.append(image)\n",
        "  return np.array(X_data)\n",
        "\n",
        "X_test = load_images(\"/content/DLTrafficCounter/data/bbox_highway/test/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im3rf-9oUBCI",
        "outputId": "99e3852d-fe47-4311-a648-8136205ef33f"
      },
      "source": [
        "WIDTH = 1920\n",
        "HEIGHT = 1080\n",
        "\n",
        "def load_labels(path):\n",
        "  files = glob_files(path, \"*.txt\")\n",
        "\n",
        "  Y_data = []\n",
        "  for file in files:\n",
        "    with open(file) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "      boxes = []\n",
        "      for line in lines:\n",
        "        tokens = line.split()\n",
        "\n",
        "        class_id = int(tokens[0])\n",
        "        xc = float(tokens[1]) * WIDTH\n",
        "        yc = float(tokens[2]) * HEIGHT\n",
        "        width = float(tokens[3]) * WIDTH\n",
        "        height = float(tokens[4]) * HEIGHT\n",
        "\n",
        "        boxes.append(np.array([class_id, xc, yc, width, height]))\n",
        "        # print(class_id, xc, yc, width, height)\n",
        "\n",
        "      Y_data.append(np.array(boxes))\n",
        "      # print(lines)\n",
        "  return np.array(Y_data)\n",
        "\n",
        "Y_test = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/test/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSayIVTsJJh2",
        "outputId": "505e31cf-f007-474a-d062-2c211e665fac"
      },
      "source": [
        "Y_test[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2.    , 1260.4032,  906.6924,  218.5344,  294.9912],\n",
              "       [   0.    , 1019.5968,  958.6512,  119.4048,  141.426 ],\n",
              "       [   0.    , 1028.7168,  868.8708,   83.6544,   84.5856],\n",
              "       [   0.    , 1451.6928,  878.2128,  123.4368,  116.1   ],\n",
              "       [   0.    , 1281.312 ,  741.1932,   71.712 ,   52.974 ],\n",
              "       [   0.    , 1369.8816,  716.6664,   73.0752,   56.8296],\n",
              "       [   0.    , 1006.6176,  625.4496,   36.288 ,   30.2724],\n",
              "       [   0.    ,  815.616 ,  606.8304,   36.4416,   31.3416],\n",
              "       [   0.    ,  740.544 ,  642.7728,   51.9168,   30.2616],\n",
              "       [   0.    , 1408.4928,  674.3196,   61.9776,   37.4436],\n",
              "       [   0.    , 1559.2128,  693.0144,   78.4128,   53.7624],\n",
              "       [   2.    , 1485.2544,  600.966 ,  124.3968,   85.4928],\n",
              "       [   0.    ,  603.9744,  689.3208,   68.4288,   36.9252],\n",
              "       [   0.    ,  644.7552,  706.0932,   62.0736,   43.7292],\n",
              "       [   2.    , 1047.0336,  693.2412,   88.8   ,  146.124 ],\n",
              "       [   0.    ,  561.312 ,  742.9104,   69.696 ,   50.5116],\n",
              "       [   2.    ,  987.3216,  583.3944,   47.3088,   66.8844]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfoGM3Tpl3vc",
        "outputId": "c2fcb053-afa3-4952-8a3a-6f253bd2f0ee"
      },
      "source": [
        "def dict_increment(dict1, key):\n",
        "  if key in dict1.keys():\n",
        "    dict1[key] = dict1[key] + 1 \n",
        "  else:\n",
        "    dict1[key] = 1\n",
        "\n",
        "  return dict1\n",
        "\n",
        "def add_dicts(dict1, dict2):\n",
        "  dict3 = dict()\n",
        "\n",
        "  for key1, val1 in dict1.items():\n",
        "    dict3[key1] = val1\n",
        "    if key1 in dict2.keys():\n",
        "      dict3[key1] = val1 + dict2[key1]\n",
        "\n",
        "  for key2, val2 in dict2.items():\n",
        "    if key2 not in dict1.keys():\n",
        "      dict3[key2] = val2\n",
        "\n",
        "  return dict3\n",
        "\n",
        "dict1 = {}\n",
        "\n",
        "dict1 = dict_increment(dict1, 'car')\n",
        "dict1 = dict_increment(dict1, 'car')\n",
        "dict1 = dict_increment(dict1, 'bus')\n",
        "dict1\n",
        "\n",
        "dict2 = {}\n",
        "dict2 = dict_increment(dict2, 'car')\n",
        "dict2 = dict_increment(dict2, 'bus')\n",
        "dict2 = dict_increment(dict2, 'truck')\n",
        "dict2 = dict_increment(dict2, 'truck')\n",
        "dict2\n",
        "\n",
        "dict3 = add_dicts(dict1, dict2)\n",
        "add_dicts(dict3, dict2)\n",
        "add_dicts(dict1, dict2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bus': 2, 'car': 3, 'truck': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrq0sGgoVrzs"
      },
      "source": [
        "def print_class_counts(dict1, class_names):\n",
        "  # print counts for each class name\n",
        "  for key, val in dict1.items():\n",
        "    print(class_names[key], val)\n",
        "\n",
        "def count_vehicles(detection_res, confidence_threshold=0.5):\n",
        "  counts = dict()\n",
        "  # print(res.names.index('car'), res.names.index('bus'), res.names.index('truck'))\n",
        "\n",
        "  for pred in detection_res.xyxyn[0]:\n",
        "    confidence = pred[-2]\n",
        "    if confidence > confidence_threshold:\n",
        "      # print(pred)\n",
        "\n",
        "      class_id = int(pred[-1])\n",
        "      counts = dict_increment(counts, class_id)\n",
        "\n",
        "  print_class_counts(counts, detection_res.names)\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBYlW6rjXytC"
      },
      "source": [
        "def count_vehicles_total(model, path, file_type=\"*.png\", confidence_threshold=0.5):\n",
        "  filenames = glob_files(path, file_type=file_type)\n",
        "  total_counts = dict()\n",
        "  class_names = None\n",
        "\n",
        "  for filename in filenames:\n",
        "    detection_res = model(filename)\n",
        "    if not class_names:\n",
        "      class_names = detection_res.names\n",
        "\n",
        "    counts = count_vehicles(detection_res,\n",
        "                            confidence_threshold=confidence_threshold)\n",
        "\n",
        "    # print(os.path.basename(filename), counts)\n",
        "    total_counts = add_dicts(total_counts, counts)\n",
        "\n",
        "  # print counts for each class name\n",
        "  print(\"\\nTotal counts:\")\n",
        "  print_class_counts(total_counts, class_names)\n",
        "\n",
        "  return total_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhqiZy53VVaW"
      },
      "source": [
        "CLASSES = ['car', 'bus', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_i0TPCQemTW",
        "outputId": "3e77d94d-c7de-46f3-b20f-640f77457c37"
      },
      "source": [
        "import yolov5\n",
        "\n",
        "model_baseline = yolov5.load('yolov5s.pt')\n",
        "count_vehicles_total(model_baseline, '/content/DLTrafficCounter/data/bbox_highway/test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "car 5\n",
            "bus 1\n",
            "car 5\n",
            "bus 1\n",
            "train 1\n",
            "car 10\n",
            "truck 4\n",
            "car 15\n",
            "truck 3\n",
            "bus 1\n",
            "bus 1\n",
            "car 13\n",
            "truck 2\n",
            "\n",
            "Total counts:\n",
            "car 48\n",
            "bus 4\n",
            "train 1\n",
            "truck 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2: 48, 5: 4, 6: 1, 7: 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPcjkcv9TClt",
        "outputId": "89a2016c-95bd-4ee0-ce23-b51e2a022baf"
      },
      "source": [
        "model_highway = yolov5.load('/content/DLTrafficCounter/models/yolov5s_highway.pt')\n",
        "count_vehicles_total(model_highway, '/content/DLTrafficCounter/data/bbox_highway/test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 5\n",
            "car 12\n",
            "car 8\n",
            "truck 7\n",
            "car 38\n",
            "truck 22\n",
            "car 47\n",
            "truck 17\n",
            "bus 3\n",
            "car 38\n",
            "truck 17\n",
            "bus 1\n",
            "\n",
            "Total counts:\n",
            "truck 68\n",
            "car 143\n",
            "bus 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 143, 1: 4, 2: 68}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aU9G7oQvwg0",
        "outputId": "e29b0855-a5f1-419f-f6fc-ff44ff7392ad"
      },
      "source": [
        "def count_vehicles_in_annotations(Y):\n",
        "  \"\"\"\n",
        "  count vehicles in the annotations\n",
        "  \"\"\"\n",
        "\n",
        "  total_counts = dict()\n",
        "  for y in Y:\n",
        "    counts = dict()\n",
        "    for ann in y:\n",
        "      counts = dict_increment(counts, int(ann[0]))\n",
        "\n",
        "    total_counts = add_dicts(total_counts, counts)\n",
        "    # print(len(y), total_counts)\n",
        "  print_class_counts(total_counts, CLASSES)\n",
        "\n",
        "count_vehicles_in_annotations(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 68\n",
            "car 147\n",
            "bus 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twtGrdjExVPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3YYfZEKxVp9"
      },
      "source": [
        "| Type  | Ground Truth  | Yolo Pretrained  | Cutom Trained |\n",
        "|---|---|---|---|\n",
        "| bus  | 10  | 4  |  4   |\n",
        "| car  | 147  | 48  |  143   |\n",
        "| truck  | 68  | 9 | 68  |\n",
        "| train  | 0  | 1 | 0  |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_fRDu3dffy",
        "outputId": "cf44bb78-20bf-4dd2-bd8d-613bbf698b7c"
      },
      "source": [
        "Y_train = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/train/\")\n",
        "count_vehicles_in_annotations(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 654\n",
            "car 1633\n",
            "bus 45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8FbJAPxdrTG",
        "outputId": "1c2abe7a-ee5d-487d-d509-c4ed7b63e0a7"
      },
      "source": [
        "Y_val = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/val/\")\n",
        "count_vehicles_in_annotations(Y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 44\n",
            "bus 2\n",
            "car 62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW_fk9bYc7YM"
      },
      "source": [
        "# Real-time inferencing\n",
        "With the vehicle counting code, we can actually test against a real stream of traffic data. Here is an example. You will see that it does not work in all cases. More data is needed to make it robust, but now you know how to do it. Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukqOv15GBF2s"
      },
      "source": [
        "# Appendix\n",
        "## To convert MS COCO xml to YOLO V5 annotation files\n",
        "Here is the code that I used to convert a single MS COCO xml file to YOLO V5 annotation files, one annotation text file for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1JVGHcnBRrY"
      },
      "source": [
        "import numpy as np\n",
        "import lxml\n",
        "\n",
        "from lxml import etree\n",
        "\n",
        "CLASSES = [\"car\", \"bus\", \"truck\"]\n",
        "\n",
        "def to_yolov5(y):\n",
        "  \"\"\"\n",
        "  # change to yolo v5 format\n",
        "  # https://github.com/ultralytics/yolov5/issues/12\n",
        "  # [x_top_left, y_top_left, x_bottom_right, y_bottom_right] to\n",
        "  # [x_center, y_center, width, height]\n",
        "  \"\"\"\n",
        "  width = y[2] - y[0]\n",
        "  height = y[3] - y[1]\n",
        "\n",
        "  if width < 0 or height < 0:\n",
        "      print(\"ERROR: negative width or height \", width, height, y)\n",
        "      raise AssertionError(\"Negative width or height\")\n",
        "  return (y[0] + (width/2)), (y[1] + (height/2)), width, height\n",
        "\n",
        "def load_xml_annotations(f):\n",
        "  tree = etree.parse(f)\n",
        "  anns = []\n",
        "  for dim in tree.xpath(\"image\"):\n",
        "    image_filename = dim.attrib[\"name\"]\n",
        "    width = int(dim.attrib[\"width\"])\n",
        "    height = int(dim.attrib[\"height\"])\n",
        "    # print(image_filename)\n",
        "    # print(len(dim.xpath(\"box\")))\n",
        "    boxes = []\n",
        "    for box in dim.xpath(\"box\"):\n",
        "      label = CLASSES.index(box.attrib[\"label\"])\n",
        "      xtl, ytl = box.attrib[\"xtl\"], box.attrib[\"ytl\"]\n",
        "      xbr, ybr = box.attrib[\"xbr\"], box.attrib[\"ybr\"]\n",
        "\n",
        "      xc, yc, w, h = to_yolov5([float(xtl), float(ytl), float(xbr), float(ybr)])\n",
        "      boxes.append([label, round(xc/width, 5), round(yc/height, 5), round(w/width, 5), round(h/height, 5)])\n",
        "\n",
        "    anns.append([image_filename, width, height, boxes])\n",
        "  \n",
        "  return np.array(anns)\n",
        "\n",
        "anns = load_xml_annotations(label_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtTNnxDWBo3O"
      },
      "source": [
        "def write_yolov5_txt(folder, annotation):\n",
        "  out_filename = folder + annotation[0][:-3] + 'txt'\n",
        "  f = open(out_filename,\"w+\")\n",
        "  for box in annotation[3]:\n",
        "    f.write(\"{} {} {} {} {}\\n\".format(box[0], box[1], box[2], box[3], box[4]))\n",
        "\n",
        "for ann in anns:\n",
        "  write_yolov5_txt(DATA_ROOT + 'train/', ann)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
