{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_as_language_teacher.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPIeCvhlOY8NMgqbONb8H7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/Medium/blob/main/yolo_as_language_teacher/language_teacher_yolov5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3IR2E-4bPaO"
      },
      "source": [
        "# Object Detection & Language Learning\n",
        "Learning language is a great challenge. It takes a long time to acquire a foreign language. Though there free language tutorials and Apps that make it easier and enjoyable, language learning is still hard. With the advancement of Deep Learning, why can't we use it help us learning a new language?\n",
        "\n",
        "Let's start with an object detection AI which can be used instead of flash cards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmr3ahqma9uk"
      },
      "source": [
        "# Setup\n",
        "Install requirements and prepare the dataset for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fNDvNWOWvpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e485c8-16e9-4858-eb70-61a6eb0c82f7"
      },
      "source": [
        "!pip install pafy\n",
        "!pip install -q youtube-dl\n",
        "\n",
        "!pip install yolov5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pafy in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: yolov5 in /usr/local/lib/python3.7/dist-packages (6.1.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.23.0)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (2.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (4.5.5.64)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.4.1)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (9.0.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.11.2)\n",
            "Requirement already satisfied: sahi>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.9.3)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.12.0+cu113)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (6.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.3.5)\n",
            "Requirement already satisfied: boto3>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from yolov5) (3.2.2)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.0.31.post2005241907)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from yolov5) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from yolov5) (1.11.0+cu113)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.5 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.19.1->yolov5) (1.26.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (1.25.11)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->yolov5) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.2->yolov5) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->yolov5) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.5->boto3>=1.19.1->yolov5) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->yolov5) (2021.10.8)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (1.8.2)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (3.1.10)\n",
            "Requirement already satisfied: click==8.0.4 in /usr/local/lib/python3.7/dist-packages (from sahi>=0.9.1->yolov5) (8.0.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click==8.0.4->sahi>=0.9.1->yolov5) (4.11.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->yolov5) (1.46.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click==8.0.4->sahi>=0.9.1->yolov5) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (3.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->yolov5) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1sEH6MGFlR"
      },
      "source": [
        "git clone Medium and yolov5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzHMg6kNuJay",
        "outputId": "6a5c78da-42f1-410c-8422-a1edb6a7f35b"
      },
      "source": [
        "!git clone https://github.com/changsin/Medium/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Medium'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 67 (delta 24), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/changsin/DLTrafficCounter/"
      ],
      "metadata": {
        "id": "Z2YIoCzRm_TR",
        "outputId": "d1167558-068c-461c-c7ab-8d34e56d6de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLTrafficCounter'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 277 (delta 17), reused 66 (delta 15), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (277/277), 226.60 MiB | 29.42 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Checking out files: 100% (205/205), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sp8S964yARl",
        "outputId": "68729142-4990-4bda-b7fa-c08d863e5122"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.11.0+cu113 (Tesla P100-PCIE-16GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccpQvXLlatKP"
      },
      "source": [
        "Download pretrained yolov5 model\n",
        "Choose one of the pretrained models from https://github.com/ultralytics/yolov5#inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5pTcJ1fyG5T",
        "outputId": "62838764-b4e5-44f2-e68e-416283283e1e"
      },
      "source": [
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5n-7-fp16.tflite"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-23 02:15:17--  https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5n-7-fp16.tflite\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-05-23 02:15:17 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1pTl7FuAVc"
      },
      "source": [
        "# YOLOv5 Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOm6wEw-rxG-",
        "outputId": "2779143d-801f-4e23-e270-3c0bf924c895"
      },
      "source": [
        "%cd yolov5"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlIHqa6lJEgy"
      },
      "source": [
        "# Detect and compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g--Y7rPweNrM"
      },
      "source": [
        "## Pretrained model (baseline)\n",
        "- YOLOV5 Default Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.5 --source ../DLTrafficCounter/data/bbox_highway/test --data ../Medium/yolo_as_language_teacher/coco128-vi.yaml"
      ],
      "metadata": {
        "id": "A-r6imfTRUiy",
        "outputId": "9f1f0338-ac2a-49ed-bcd5-5d112253d01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=../DLTrafficCounter/data/bbox_highway/test, data=../Medium/yolo_as_language_teacher/coco128-vi.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 üöÄ v6.1-211-gcee5959 Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "['ng∆∞·ªùi', 'Xe ƒë·∫°p', 'xe √¥ t√¥', 'xe m√°y', 'M√°y bay', 'xe bu√Ωt', 't√†u h·ªèa', 'xe t·∫£i', 'thuy·ªÅn', 'ƒë√®n giao th√¥ng', 'v√≤i ch·ªØa ch√°y', 'bi·ªÉn b√°o d·ª´ng', 'ƒê·ªìng h·ªì ƒë·ªó xe', 'BƒÉng gh·∫ø', 'chim', 'con m√®o', 'ch√∫ ch√≥', 'con ng·ª±a', 'con c·ª´u', 'b√≤', 'con voi', 'con g·∫•u', 'ng·ª±a r·∫±n', 'h∆∞∆°u cao c·ªï', 'balo', '√¥', 't√∫i x√°ch tay', 'c√† v·∫°t', 'chi·∫øc vali', 'chi·∫øc dƒ©a nh·ª±a n√©m', 'v√°n tr∆∞·ª£t', 'tr∆∞·ª£t tuy·∫øt', 'b√≥ng th·ªÉ thao', 'c√°nh di·ªÅu', 'g·∫≠y b√≥ng ch√†y', 'gƒÉng tay b√≥ng ch√†y', 'v√°n tr∆∞·ª£t', 'v√°n l∆∞·ªõt s√≥ng', 'v·ª£t tennis', 'chai', 'ly r∆∞·ª£u', 't√°ch', 'c√°i nƒ©a', 'dao', 'c√°i th√¨a', 'b√°t', 'tr√°i chu·ªëi', 't√°o', 'b√°nh m√¨ sandwich', 'tr√°i cam', 'b√¥ng c·∫£i xanh', 'C√† r·ªët', 'b√°nh m√¨ k·∫πp x√∫c x√≠ch', 'pizza', 'b√°nh v√≤ng', 'b√°nh', 'c√°i gh·∫ø', 'ƒëi vƒÉng', 'c√¢y ch·∫≠u', 'Gi∆∞·ªùng', 'b√†n ƒÉn', 'ph√≤ng v·ªá sinh', 'TV', 'm√°y t√≠nh x√°ch tay', 'con chu·ªôt', 'Xa x√¥i', 'b√†n ph√≠m', 'ƒëi√™Ã£n thoaÃ£i di ƒë√¥Ã£ng', 'l√≤ vi s√≥ng', 'l√≤', 'M√°y n∆∞·ªõng b√°nh m√¨', 'b·ªìn r·ª≠a ch√©n', 't·ªß l·∫°nh', 's√°ch', 'ƒë·ªìng h·ªì', 'l·ªç c·∫Øm hoa', 'c√¢y k√©o', 'g·∫•u b√¥ng', 'm√°y s·∫•y t√≥c', 'B√†n ch·∫£i ƒë√°nh rƒÉng']\n",
            "Fusing layers... \n",
            "Model summary: 224 layers, 7266973 parameters, 0 gradients\n",
            "#####pt model.names ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
            "*****['ng∆∞·ªùi', 'Xe ƒë·∫°p', 'xe √¥ t√¥', 'xe m√°y', 'M√°y bay', 'xe bu√Ωt', 't√†u h·ªèa', 'xe t·∫£i', 'thuy·ªÅn', 'ƒë√®n giao th√¥ng', 'v√≤i ch·ªØa ch√°y', 'bi·ªÉn b√°o d·ª´ng', 'ƒê·ªìng h·ªì ƒë·ªó xe', 'BƒÉng gh·∫ø', 'chim', 'con m√®o', 'ch√∫ ch√≥', 'con ng·ª±a', 'con c·ª´u', 'b√≤', 'con voi', 'con g·∫•u', 'ng·ª±a r·∫±n', 'h∆∞∆°u cao c·ªï', 'balo', '√¥', 't√∫i x√°ch tay', 'c√† v·∫°t', 'chi·∫øc vali', 'chi·∫øc dƒ©a nh·ª±a n√©m', 'v√°n tr∆∞·ª£t', 'tr∆∞·ª£t tuy·∫øt', 'b√≥ng th·ªÉ thao', 'c√°nh di·ªÅu', 'g·∫≠y b√≥ng ch√†y', 'gƒÉng tay b√≥ng ch√†y', 'v√°n tr∆∞·ª£t', 'v√°n l∆∞·ªõt s√≥ng', 'v·ª£t tennis', 'chai', 'ly r∆∞·ª£u', 't√°ch', 'c√°i nƒ©a', 'dao', 'c√°i th√¨a', 'b√°t', 'tr√°i chu·ªëi', 't√°o', 'b√°nh m√¨ sandwich', 'tr√°i cam', 'b√¥ng c·∫£i xanh', 'C√† r·ªët', 'b√°nh m√¨ k·∫πp x√∫c x√≠ch', 'pizza', 'b√°nh v√≤ng', 'b√°nh', 'c√°i gh·∫ø', 'ƒëi vƒÉng', 'c√¢y ch·∫≠u', 'Gi∆∞·ªùng', 'b√†n ƒÉn', 'ph√≤ng v·ªá sinh', 'TV', 'm√°y t√≠nh x√°ch tay', 'con chu·ªôt', 'Xa x√¥i', 'b√†n ph√≠m', 'ƒëi√™Ã£n thoaÃ£i di ƒë√¥Ã£ng', 'l√≤ vi s√≥ng', 'l√≤', 'M√°y n∆∞·ªõng b√°nh m√¨', 'b·ªìn r·ª≠a ch√©n', 't·ªß l·∫°nh', 's√°ch', 'ƒë·ªìng h·ªì', 'l·ªç c·∫Øm hoa', 'c√¢y k√©o', 'g·∫•u b√¥ng', 'm√°y s·∫•y t√≥c', 'B√†n ch·∫£i ƒë√°nh rƒÉng']\n",
            "$$$$$$['ng∆∞·ªùi', 'Xe ƒë·∫°p', 'xe √¥ t√¥', 'xe m√°y', 'M√°y bay', 'xe bu√Ωt', 't√†u h·ªèa', 'xe t·∫£i', 'thuy·ªÅn', 'ƒë√®n giao th√¥ng', 'v√≤i ch·ªØa ch√°y', 'bi·ªÉn b√°o d·ª´ng', 'ƒê·ªìng h·ªì ƒë·ªó xe', 'BƒÉng gh·∫ø', 'chim', 'con m√®o', 'ch√∫ ch√≥', 'con ng·ª±a', 'con c·ª´u', 'b√≤', 'con voi', 'con g·∫•u', 'ng·ª±a r·∫±n', 'h∆∞∆°u cao c·ªï', 'balo', '√¥', 't√∫i x√°ch tay', 'c√† v·∫°t', 'chi·∫øc vali', 'chi·∫øc dƒ©a nh·ª±a n√©m', 'v√°n tr∆∞·ª£t', 'tr∆∞·ª£t tuy·∫øt', 'b√≥ng th·ªÉ thao', 'c√°nh di·ªÅu', 'g·∫≠y b√≥ng ch√†y', 'gƒÉng tay b√≥ng ch√†y', 'v√°n tr∆∞·ª£t', 'v√°n l∆∞·ªõt s√≥ng', 'v·ª£t tennis', 'chai', 'ly r∆∞·ª£u', 't√°ch', 'c√°i nƒ©a', 'dao', 'c√°i th√¨a', 'b√°t', 'tr√°i chu·ªëi', 't√°o', 'b√°nh m√¨ sandwich', 'tr√°i cam', 'b√¥ng c·∫£i xanh', 'C√† r·ªët', 'b√°nh m√¨ k·∫πp x√∫c x√≠ch', 'pizza', 'b√°nh v√≤ng', 'b√°nh', 'c√°i gh·∫ø', 'ƒëi vƒÉng', 'c√¢y ch·∫≠u', 'Gi∆∞·ªùng', 'b√†n ƒÉn', 'ph√≤ng v·ªá sinh', 'TV', 'm√°y t√≠nh x√°ch tay', 'con chu·ªôt', 'Xa x√¥i', 'b√†n ph√≠m', 'ƒëi√™Ã£n thoaÃ£i di ƒë√¥Ã£ng', 'l√≤ vi s√≥ng', 'l√≤', 'M√°y n∆∞·ªõng b√°nh m√¨', 'b·ªìn r·ª≠a ch√©n', 't·ªß l·∫°nh', 's√°ch', 'ƒë·ªìng h·ªì', 'l·ªç c·∫Øm hoa', 'c√¢y k√©o', 'g·∫•u b√¥ng', 'm√°y s·∫•y t√≥c', 'B√†n ch·∫£i ƒë√°nh rƒÉng']\n",
            "Downloading https://ultralytics.com/assets/Arial.Unicode.ttf to /root/.config/Ultralytics/Arial.Unicode.ttf...\n",
            "image 1/5 /content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_096.png: 384x640 5 xe √¥ t√¥s, 1 xe bu√Ωt, Done. (0.019s)\n",
            "image 2/5 /content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_097.png: 384x640 5 xe √¥ t√¥s, 1 xe bu√Ωt, 1 t√†u h·ªèa, 1 xe t·∫£i, Done. (0.012s)\n",
            "image 3/5 /content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_098.png: 384x640 10 xe √¥ t√¥s, 4 xe t·∫£is, Done. (0.011s)\n",
            "image 4/5 /content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_099.png: 384x640 15 xe √¥ t√¥s, 1 xe bu√Ωt, 3 xe t·∫£is, Done. (0.011s)\n",
            "image 5/5 /content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_100.png: 384x640 13 xe √¥ t√¥s, 1 xe bu√Ωt, 2 xe t·∫£is, Done. (0.011s)\n",
            "Speed: 0.4ms pre-process, 12.9ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp17\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePHBhTXUJbRZ"
      },
      "source": [
        "- Run against the customized and better vehicle detection model.\n",
        "(If you are running yourself, you need to modify the path of the weights file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MswmiNkiR_ON"
      },
      "source": [
        "# Detect and Count\n",
        "To detect and count each vehicle type, we need to parse the detection results returned by YOLO. Here is a brief explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL5gKdpA_VCg"
      },
      "source": [
        "## Explanation of detection results\n",
        "\n",
        "1. results.names contain the names of classes: e.g., 'person'. There are 80 of them by default corresponding to 80 COCO dataset classes.\n",
        "\n",
        "  ['person',\n",
        " 'bicycle',\n",
        " 'car',\n",
        " 'motorcycle',\n",
        " 'airplane',\n",
        " 'bus',\n",
        " 'train',\n",
        " 'truck',\n",
        " 'boat',\n",
        " 'traffic light',\n",
        " 'fire hydrant',\n",
        " 'stop sign',\n",
        " 'parking meter',\n",
        " 'bench',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'dog',\n",
        " 'horse',\n",
        " 'sheep',\n",
        " 'cow',\n",
        " 'elephant',\n",
        " 'bear',\n",
        " 'zebra',\n",
        " 'giraffe',\n",
        " 'backpack',\n",
        " 'umbrella',\n",
        " 'handbag',\n",
        " 'tie',\n",
        " 'suitcase',\n",
        " 'frisbee',\n",
        " 'skis',\n",
        " 'snowboard',\n",
        " 'sports ball',\n",
        " 'kite',\n",
        " 'baseball bat',\n",
        " 'baseball glove',\n",
        " 'skateboard',\n",
        " 'surfboard',\n",
        " 'tennis racket',\n",
        " 'bottle',\n",
        " 'wine glass',\n",
        " 'cup',\n",
        " 'fork',\n",
        " 'knife',\n",
        " 'spoon',\n",
        " 'bowl',\n",
        " 'banana',\n",
        " 'apple',\n",
        " 'sandwich',\n",
        " 'orange',\n",
        " 'broccoli',\n",
        " 'carrot',\n",
        " 'hot dog',\n",
        " 'pizza',\n",
        " 'donut',\n",
        " 'cake',\n",
        " 'chair',\n",
        " 'couch',\n",
        " 'potted plant',\n",
        " 'bed',\n",
        " 'dining table',\n",
        " 'toilet',\n",
        " 'tv',\n",
        " 'laptop',\n",
        " 'mouse',\n",
        " 'remote',\n",
        " 'keyboard',\n",
        " 'cell phone',\n",
        " 'microwave',\n",
        " 'oven',\n",
        " 'toaster',\n",
        " 'sink',\n",
        " 'refrigerator',\n",
        " 'book',\n",
        " 'clock',\n",
        " 'vase',\n",
        " 'scissors',\n",
        " 'teddy bear',\n",
        " 'hair drier',\n",
        " 'toothbrush']\n",
        " \n",
        "\n",
        "2. results.xyxyn: xy coordinates followed by the confidence and the class id. For instance, the first item is class_id=0 with 90% confidence which refers to 'person' class\n",
        "\n",
        "```\n",
        "[tensor([[ 0.73203,  0.43620,  0.85469,  0.88646,  0.90088,  0.00000],\n",
        "         [ 0.70586,  0.36276,  0.92344,  0.49609,  0.62939, 25.00000],\n",
        "         [ 0.58125,  0.40365,  0.73984,  0.78594,  0.46143, 77.00000],\n",
        "         [ 0.39355,  0.15990,  0.58789,  0.80365,  0.44385, 10.00000],\n",
        "         [ 0.19248,  0.50104,  0.20469,  0.54062,  0.29517,  0.00000]], device='cuda:0')]\n",
        "```\n",
        "results.xyxy and results.pred have the same content except in scientific notations.\n",
        "```\n",
        "[tensor([[1.75687e+03, 7.85156e+02, 2.05125e+03, 1.59562e+03, 9.00879e-01, 0.00000e+00],\n",
        "         [1.69406e+03, 6.52969e+02, 2.21625e+03, 8.92969e+02, 6.29395e-01, 2.50000e+01],\n",
        "         [1.39500e+03, 7.26562e+02, 1.77562e+03, 1.41469e+03, 4.61426e-01, 7.70000e+01],\n",
        "         [9.44531e+02, 2.87812e+02, 1.41094e+03, 1.44656e+03, 4.43848e-01, 1.00000e+01],\n",
        "         [4.61953e+02, 9.01875e+02, 4.91250e+02, 9.73125e+02, 2.95166e-01, 0.00000e+00]], device='cuda:0')]\n",
        "```\n",
        "3. results.imgs is the labeled image containing the detection results.\n",
        "4. results.save('folder') saves the detection result image to the folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t5lmfBT_6jl"
      },
      "source": [
        "With this information, we can now parse and count each vehicle type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8xTskHP5jAy"
      },
      "source": [
        "## Plot annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJnZzGK2bh6F"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "def glob_files(path, file_type=\"*\"):\n",
        "    search_string = os.path.join(path, file_type)\n",
        "    files = glob.glob(search_string)\n",
        "\n",
        "    # print('searching ', path)\n",
        "    paths = []\n",
        "    for f in files:\n",
        "      if os.path.isdir(f):\n",
        "        sub_paths = glob_files(f + '/')\n",
        "        paths += sub_paths\n",
        "      else:\n",
        "        paths.append(f)\n",
        "\n",
        "    # We sort the images in alphabetical order to match them\n",
        "    #  to the annotation files\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "92pOmQOtP9_Y",
        "outputId": "7a5ef52a-75e3-4599-be67-16649036a648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow==9.0.0 in /usr/local/lib/python3.7/dist-packages (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/DLTrafficCounter/data/bbox_highway/test/\"\n",
        "\n",
        "image_filenames = glob_files(path, file_type=\"*.png\")\n",
        "image_filenames"
      ],
      "metadata": {
        "id": "KcB-u8pIQvKD",
        "outputId": "3a6334fa-6377-40c6-cffd-d023db07db4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_096.png',\n",
              " '/content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_097.png',\n",
              " '/content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_098.png',\n",
              " '/content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_099.png',\n",
              " '/content/DLTrafficCounter/data/bbox_highway/test/Suwon_CH02_20200722_1600_WED_9m_RH_highway_TW5_rainy_FHD_100.png']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_i0TPCQemTW",
        "outputId": "770aa77d-35e7-442b-f39f-bf21559a31c3"
      },
      "source": [
        "import yolov5\n",
        "\n",
        "model = yolov5.load('yolov5s.pt')\n",
        "\n",
        "pred = model(image_filenames[0])\n",
        "pred\n",
        "\n",
        "# def count_vehicles_total(model, path, file_type=\"*.png\", confidence_threshold=0.5):\n",
        "#   filenames = glob_files(path, file_type=file_type)\n",
        "#   total_counts = dict()\n",
        "#   class_names = None\n",
        "\n",
        "#   for filename in filenames:\n",
        "#     detection_res = model(filename)\n",
        "#     if not class_names:\n",
        "#       class_names = detection_res.names\n",
        "\n",
        "#     counts = count_vehicles(detection_res,\n",
        "#                             confidence_threshold=confidence_threshold)\n",
        "\n",
        "#     # print(os.path.basename(filename), counts)\n",
        "#     total_counts = add_dicts(total_counts, counts)\n",
        "\n",
        "#   # print counts for each class name\n",
        "#   print(\"\\nTotal counts:\")\n",
        "#   print_class_counts(total_counts, class_names)\n",
        "\n",
        "#   return total_counts\n",
        "# # count_vehicles_total(model_baseline, '/content/DLTrafficCounter/data/bbox_highway/test')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<yolov5.models.common.Detections at 0x7fb9ffbfe6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred.names)"
      ],
      "metadata": {
        "id": "hl0UzvnqRpep",
        "outputId": "94e60c91-14e0-416f-8842-0a7ab8483d1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Medium/yolo_as_language_teacher/coco128-vi.yaml"
      ],
      "metadata": {
        "id": "-1G4X4dCSYxc",
        "outputId": "65bbbedb-a575-49b7-e3be-8271484d49a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medium/yolo_as_language_teacher/coco128-vi.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Vietnamese class names from the yaml file"
      ],
      "metadata": {
        "id": "FvQrBeVuaLrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "filename_coco128_vn = '/content/Medium/yolo_as_language_teacher/coco128-vi.yaml'\n",
        "coco_vn = yaml.safe_load(Path(filename_coco128_vn).read_text())"
      ],
      "metadata": {
        "id": "lfssrWaoSUlg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.names"
      ],
      "metadata": {
        "id": "q1OE23LDWpLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_vn['names']"
      ],
      "metadata": {
        "id": "HSEUZ4ISWA14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a dictionary out of English and Vietnamese class names"
      ],
      "metadata": {
        "id": "OxqouBZ3aWJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coco_classes_en_vi = dict(zip(pred.names, coco_vn['names']))"
      ],
      "metadata": {
        "id": "05k-eOZRZG4Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in coco_classes_en_vi.items():\n",
        "  print(\"{}: {}\".format(k, v))"
      ],
      "metadata": {
        "id": "lbd7XVPFYmb5",
        "outputId": "63660612-ede0-46cc-d3cf-8365e322bec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person: ng∆∞·ªùi\n",
            "bicycle: Xe ƒë·∫°p\n",
            "car: xe √¥ t√¥\n",
            "motorcycle: xe m√°y\n",
            "airplane: M√°y bay\n",
            "bus: xe bu√Ωt\n",
            "train: t√†u h·ªèa\n",
            "truck: xe t·∫£i\n",
            "boat: thuy·ªÅn\n",
            "traffic light: ƒë√®n giao th√¥ng\n",
            "fire hydrant: v√≤i ch·ªØa ch√°y\n",
            "stop sign: bi·ªÉn b√°o d·ª´ng\n",
            "parking meter: ƒê·ªìng h·ªì ƒë·ªó xe\n",
            "bench: BƒÉng gh·∫ø\n",
            "bird: chim\n",
            "cat: con m√®o\n",
            "dog: ch√∫ ch√≥\n",
            "horse: con ng·ª±a\n",
            "sheep: con c·ª´u\n",
            "cow: b√≤\n",
            "elephant: con voi\n",
            "bear: con g·∫•u\n",
            "zebra: ng·ª±a r·∫±n\n",
            "giraffe: h∆∞∆°u cao c·ªï\n",
            "backpack: balo\n",
            "umbrella: √¥\n",
            "handbag: t√∫i x√°ch tay\n",
            "tie: c√† v·∫°t\n",
            "suitcase: chi·∫øc vali\n",
            "frisbee: chi·∫øc dƒ©a nh·ª±a n√©m\n",
            "skis: v√°n tr∆∞·ª£t\n",
            "snowboard: tr∆∞·ª£t tuy·∫øt\n",
            "sports ball: b√≥ng th·ªÉ thao\n",
            "kite: c√°nh di·ªÅu\n",
            "baseball bat: g·∫≠y b√≥ng ch√†y\n",
            "baseball glove: gƒÉng tay b√≥ng ch√†y\n",
            "skateboard: v√°n tr∆∞·ª£t\n",
            "surfboard: v√°n l∆∞·ªõt s√≥ng\n",
            "tennis racket: v·ª£t tennis\n",
            "bottle: chai\n",
            "wine glass: ly r∆∞·ª£u\n",
            "cup: t√°ch\n",
            "fork: c√°i nƒ©a\n",
            "knife: dao\n",
            "spoon: c√°i th√¨a\n",
            "bowl: b√°t\n",
            "banana: tr√°i chu·ªëi\n",
            "apple: t√°o\n",
            "sandwich: b√°nh m√¨ sandwich\n",
            "orange: tr√°i cam\n",
            "broccoli: b√¥ng c·∫£i xanh\n",
            "carrot: C√† r·ªët\n",
            "hot dog: b√°nh m√¨ k·∫πp x√∫c x√≠ch\n",
            "pizza: pizza\n",
            "donut: b√°nh v√≤ng\n",
            "cake: b√°nh\n",
            "chair: c√°i gh·∫ø\n",
            "couch: ƒëi vƒÉng\n",
            "potted plant: c√¢y ch·∫≠u\n",
            "bed: Gi∆∞·ªùng\n",
            "dining table: b√†n ƒÉn\n",
            "toilet: ph√≤ng v·ªá sinh\n",
            "tv: TV\n",
            "laptop: m√°y t√≠nh x√°ch tay\n",
            "mouse: con chu·ªôt\n",
            "remote: Xa x√¥i\n",
            "keyboard: b√†n ph√≠m\n",
            "cell phone: ƒëi√™Ã£n thoaÃ£i di ƒë√¥Ã£ng\n",
            "microwave: l√≤ vi s√≥ng\n",
            "oven: l√≤\n",
            "toaster: M√°y n∆∞·ªõng b√°nh m√¨\n",
            "sink: b·ªìn r·ª≠a ch√©n\n",
            "refrigerator: t·ªß l·∫°nh\n",
            "book: s√°ch\n",
            "clock: ƒë·ªìng h·ªì\n",
            "vase: l·ªç c·∫Øm hoa\n",
            "scissors: c√¢y k√©o\n",
            "teddy bear: g·∫•u b√¥ng\n",
            "hair drier: m√°y s·∫•y t√≥c\n",
            "toothbrush: B√†n ch·∫£i ƒë√°nh rƒÉng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ_C0gh4TdbZ"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "IMAGE_SIZE = 600\n",
        "\n",
        "def load_images(path):\n",
        "  files = glob_files(path, \"*.png\")\n",
        "\n",
        "  # print(files)\n",
        "  X_data = []\n",
        "  for file in files:\n",
        "    image = cv2.imread(file)\n",
        "    # print(image.shape)\n",
        "    # x = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    X_data.append(image)\n",
        "  return np.array(X_data)\n",
        "\n",
        "X_test = load_images(\"/content/DLTrafficCounter/data/bbox_highway/test/\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im3rf-9oUBCI",
        "outputId": "d267f351-9f34-431d-e227-37307e779a3d"
      },
      "source": [
        "WIDTH = 1920\n",
        "HEIGHT = 1080\n",
        "\n",
        "def load_labels(path):\n",
        "  files = glob_files(path, \"*.txt\")\n",
        "\n",
        "  Y_data = []\n",
        "  for file in files:\n",
        "    with open(file) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "      boxes = []\n",
        "      for line in lines:\n",
        "        tokens = line.split()\n",
        "\n",
        "        class_id = int(tokens[0])\n",
        "        xc = float(tokens[1]) * WIDTH\n",
        "        yc = float(tokens[2]) * HEIGHT\n",
        "        width = float(tokens[3]) * WIDTH\n",
        "        height = float(tokens[4]) * HEIGHT\n",
        "\n",
        "        boxes.append(np.array([class_id, xc, yc, width, height]))\n",
        "        # print(class_id, xc, yc, width, height)\n",
        "\n",
        "      Y_data.append(np.array(boxes))\n",
        "      # print(lines)\n",
        "  return np.array(Y_data)\n",
        "\n",
        "Y_test = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/test/\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSayIVTsJJh2",
        "outputId": "80e83fb0-9e74-495a-edbf-aae7c93493e8"
      },
      "source": [
        "Y_test[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2.    , 1260.4032,  906.6924,  218.5344,  294.9912],\n",
              "       [   0.    , 1019.5968,  958.6512,  119.4048,  141.426 ],\n",
              "       [   0.    , 1028.7168,  868.8708,   83.6544,   84.5856],\n",
              "       [   0.    , 1451.6928,  878.2128,  123.4368,  116.1   ],\n",
              "       [   0.    , 1281.312 ,  741.1932,   71.712 ,   52.974 ],\n",
              "       [   0.    , 1369.8816,  716.6664,   73.0752,   56.8296],\n",
              "       [   0.    , 1006.6176,  625.4496,   36.288 ,   30.2724],\n",
              "       [   0.    ,  815.616 ,  606.8304,   36.4416,   31.3416],\n",
              "       [   0.    ,  740.544 ,  642.7728,   51.9168,   30.2616],\n",
              "       [   0.    , 1408.4928,  674.3196,   61.9776,   37.4436],\n",
              "       [   0.    , 1559.2128,  693.0144,   78.4128,   53.7624],\n",
              "       [   2.    , 1485.2544,  600.966 ,  124.3968,   85.4928],\n",
              "       [   0.    ,  603.9744,  689.3208,   68.4288,   36.9252],\n",
              "       [   0.    ,  644.7552,  706.0932,   62.0736,   43.7292],\n",
              "       [   2.    , 1047.0336,  693.2412,   88.8   ,  146.124 ],\n",
              "       [   0.    ,  561.312 ,  742.9104,   69.696 ,   50.5116],\n",
              "       [   2.    ,  987.3216,  583.3944,   47.3088,   66.8844]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfoGM3Tpl3vc",
        "outputId": "1b34f9e5-da11-444f-8dae-bf6a4474f9b9"
      },
      "source": [
        "def dict_increment(dict1, key):\n",
        "  if key in dict1.keys():\n",
        "    dict1[key] = dict1[key] + 1 \n",
        "  else:\n",
        "    dict1[key] = 1\n",
        "\n",
        "  return dict1\n",
        "\n",
        "def add_dicts(dict1, dict2):\n",
        "  dict3 = dict()\n",
        "\n",
        "  for key1, val1 in dict1.items():\n",
        "    dict3[key1] = val1\n",
        "    if key1 in dict2.keys():\n",
        "      dict3[key1] = val1 + dict2[key1]\n",
        "\n",
        "  for key2, val2 in dict2.items():\n",
        "    if key2 not in dict1.keys():\n",
        "      dict3[key2] = val2\n",
        "\n",
        "  return dict3\n",
        "\n",
        "dict1 = {}\n",
        "\n",
        "dict1 = dict_increment(dict1, 'car')\n",
        "dict1 = dict_increment(dict1, 'car')\n",
        "dict1 = dict_increment(dict1, 'bus')\n",
        "dict1\n",
        "\n",
        "dict2 = {}\n",
        "dict2 = dict_increment(dict2, 'car')\n",
        "dict2 = dict_increment(dict2, 'bus')\n",
        "dict2 = dict_increment(dict2, 'truck')\n",
        "dict2 = dict_increment(dict2, 'truck')\n",
        "dict2\n",
        "\n",
        "dict3 = add_dicts(dict1, dict2)\n",
        "add_dicts(dict3, dict2)\n",
        "add_dicts(dict1, dict2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bus': 2, 'car': 3, 'truck': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrq0sGgoVrzs"
      },
      "source": [
        "def print_class_counts(dict1, class_names):\n",
        "  # print counts for each class name\n",
        "  for key, val in dict1.items():\n",
        "    print(class_names[key], val)\n",
        "\n",
        "def count_vehicles(detection_res, confidence_threshold=0.5):\n",
        "  counts = dict()\n",
        "  print(detection_res.names.index('car'),\n",
        "        detection_res.names.index('bus'),\n",
        "        detection_res.names.index('truck'))\n",
        "\n",
        "  for pred in detection_res.xyxyn[0]:\n",
        "    confidence = pred[-2]\n",
        "    if confidence > confidence_threshold:\n",
        "      # print(pred)\n",
        "\n",
        "      class_id = int(pred[-1])\n",
        "      counts = dict_increment(counts, class_id)\n",
        "\n",
        "  print_class_counts(counts, detection_res.names)\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBYlW6rjXytC"
      },
      "source": [
        "def count_vehicles_total(model, path, file_type=\"*.png\", confidence_threshold=0.5):\n",
        "  filenames = glob_files(path, file_type=file_type)\n",
        "  total_counts = dict()\n",
        "  class_names = None\n",
        "\n",
        "  for filename in filenames:\n",
        "    detection_res = model(filename)\n",
        "    if not class_names:\n",
        "      class_names = detection_res.names\n",
        "\n",
        "    counts = count_vehicles(detection_res,\n",
        "                            confidence_threshold=confidence_threshold)\n",
        "\n",
        "    # print(os.path.basename(filename), counts)\n",
        "    total_counts = add_dicts(total_counts, counts)\n",
        "\n",
        "  # print counts for each class name\n",
        "  print(\"\\nTotal counts:\")\n",
        "  print_class_counts(total_counts, class_names)\n",
        "\n",
        "  return total_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhqiZy53VVaW"
      },
      "source": [
        "CLASSES = ['car', 'bus', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPcjkcv9TClt",
        "outputId": "89a2016c-95bd-4ee0-ce23-b51e2a022baf"
      },
      "source": [
        "model_highway = yolov5.load('/content/DLTrafficCounter/models/yolov5s_highway.pt')\n",
        "count_vehicles_total(model_highway, '/content/DLTrafficCounter/data/bbox_highway/test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 5\n",
            "car 12\n",
            "car 8\n",
            "truck 7\n",
            "car 38\n",
            "truck 22\n",
            "car 47\n",
            "truck 17\n",
            "bus 3\n",
            "car 38\n",
            "truck 17\n",
            "bus 1\n",
            "\n",
            "Total counts:\n",
            "truck 68\n",
            "car 143\n",
            "bus 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 143, 1: 4, 2: 68}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aU9G7oQvwg0",
        "outputId": "e29b0855-a5f1-419f-f6fc-ff44ff7392ad"
      },
      "source": [
        "def count_vehicles_in_annotations(Y):\n",
        "  \"\"\"\n",
        "  count vehicles in the annotations\n",
        "  \"\"\"\n",
        "\n",
        "  total_counts = dict()\n",
        "  for y in Y:\n",
        "    counts = dict()\n",
        "    for ann in y:\n",
        "      counts = dict_increment(counts, int(ann[0]))\n",
        "\n",
        "    total_counts = add_dicts(total_counts, counts)\n",
        "    # print(len(y), total_counts)\n",
        "  print_class_counts(total_counts, CLASSES)\n",
        "\n",
        "count_vehicles_in_annotations(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 68\n",
            "car 147\n",
            "bus 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twtGrdjExVPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3YYfZEKxVp9"
      },
      "source": [
        "| Type  | Ground Truth  | Yolo Pretrained  | Cutom Trained |\n",
        "|---|---|---|---|\n",
        "| bus  | 10  | 4  |  4   |\n",
        "| car  | 147  | 48  |  143   |\n",
        "| truck  | 68  | 9 | 68  |\n",
        "| train  | 0  | 1 | 0  |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_fRDu3dffy",
        "outputId": "cf44bb78-20bf-4dd2-bd8d-613bbf698b7c"
      },
      "source": [
        "Y_train = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/train/\")\n",
        "count_vehicles_in_annotations(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 654\n",
            "car 1633\n",
            "bus 45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8FbJAPxdrTG",
        "outputId": "1c2abe7a-ee5d-487d-d509-c4ed7b63e0a7"
      },
      "source": [
        "Y_val = load_labels(\"/content/DLTrafficCounter/data/bbox_highway/val/\")\n",
        "count_vehicles_in_annotations(Y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck 44\n",
            "bus 2\n",
            "car 62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW_fk9bYc7YM"
      },
      "source": [
        "# Real-time inferencing\n",
        "With the vehicle counting code, we can actually test against a real stream of traffic data. Here is an example. You will see that it does not work in all cases. More data is needed to make it robust, but now you know how to do it. Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukqOv15GBF2s"
      },
      "source": [
        "# Appendix\n",
        "## To convert MS COCO xml to YOLO V5 annotation files\n",
        "Here is the code that I used to convert a single MS COCO xml file to YOLO V5 annotation files, one annotation text file for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1JVGHcnBRrY"
      },
      "source": [
        "import numpy as np\n",
        "import lxml\n",
        "\n",
        "from lxml import etree\n",
        "\n",
        "CLASSES = [\"car\", \"bus\", \"truck\"]\n",
        "\n",
        "def to_yolov5(y):\n",
        "  \"\"\"\n",
        "  # change to yolo v5 format\n",
        "  # https://github.com/ultralytics/yolov5/issues/12\n",
        "  # [x_top_left, y_top_left, x_bottom_right, y_bottom_right] to\n",
        "  # [x_center, y_center, width, height]\n",
        "  \"\"\"\n",
        "  width = y[2] - y[0]\n",
        "  height = y[3] - y[1]\n",
        "\n",
        "  if width < 0 or height < 0:\n",
        "      print(\"ERROR: negative width or height \", width, height, y)\n",
        "      raise AssertionError(\"Negative width or height\")\n",
        "  return (y[0] + (width/2)), (y[1] + (height/2)), width, height\n",
        "\n",
        "def load_xml_annotations(f):\n",
        "  tree = etree.parse(f)\n",
        "  anns = []\n",
        "  for dim in tree.xpath(\"image\"):\n",
        "    image_filename = dim.attrib[\"name\"]\n",
        "    width = int(dim.attrib[\"width\"])\n",
        "    height = int(dim.attrib[\"height\"])\n",
        "    # print(image_filename)\n",
        "    # print(len(dim.xpath(\"box\")))\n",
        "    boxes = []\n",
        "    for box in dim.xpath(\"box\"):\n",
        "      label = CLASSES.index(box.attrib[\"label\"])\n",
        "      xtl, ytl = box.attrib[\"xtl\"], box.attrib[\"ytl\"]\n",
        "      xbr, ybr = box.attrib[\"xbr\"], box.attrib[\"ybr\"]\n",
        "\n",
        "      xc, yc, w, h = to_yolov5([float(xtl), float(ytl), float(xbr), float(ybr)])\n",
        "      boxes.append([label, round(xc/width, 5), round(yc/height, 5), round(w/width, 5), round(h/height, 5)])\n",
        "\n",
        "    anns.append([image_filename, width, height, boxes])\n",
        "  \n",
        "  return np.array(anns)\n",
        "\n",
        "anns = load_xml_annotations(label_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtTNnxDWBo3O"
      },
      "source": [
        "def write_yolov5_txt(folder, annotation):\n",
        "  out_filename = folder + annotation[0][:-3] + 'txt'\n",
        "  f = open(out_filename,\"w+\")\n",
        "  for box in annotation[3]:\n",
        "    f.write(\"{} {} {} {} {}\\n\".format(box[0], box[1], box[2], box[3], box[4]))\n",
        "\n",
        "for ann in anns:\n",
        "  write_yolov5_txt(DATA_ROOT + 'train/', ann)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}