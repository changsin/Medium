{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbWd4zQH7ByO"
      },
      "source": [
        "# Numbers every developer should know about prompt engineering\n",
        "\n",
        "A developer needs to know a few core numbers to be effective and efficient. Memory footprint, the performance of a program, the latency, or bandwidth of a network come to mind. With the advent of AI, especially Generative AI like LLMs, prompt engineering is not resigned for just prompt engineers. Copilot has already been used by every developer I know. That prompted (pun not intended) the question of the article: what are the numbers that a developer should know about prompt engineering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j_YS_j-6RJk"
      },
      "source": [
        "## Prepare sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2f-JDWc4y1e",
        "outputId": "d180293f-02a4-4366-9c47-ba834608fe5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file 'ai_agent_data.csv' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# I want to first prepare some data. It's mimicking prompt/response for an AI agent.\n",
        "# Query sentences followed by the actual responses from AI and the expected responses (ground truth).\n",
        "# I want 10 such pairs in csv format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zPX-nMXGe0T"
      },
      "source": [
        "## Set OPEN_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4KXBgpJH8nSp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_sM731H6YpB"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmcyvoCu8Q6P"
      },
      "source": [
        "## What is a token?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "935vAHX28WEw"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIoTKxdp9g8y",
        "outputId": "c7402c6b-578f-48a4-ac99-15696d551718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10516, 4827, 15821]\n",
            "['low', ' lower', ' lowest']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    enc = tiktoken.encoding_for_model(encoding_name)\n",
        "    tokens = enc.encode(string)\n",
        "    print(tokens)                         # → [11455, 15639, 13959]\n",
        "    print([enc.decode([t]) for t in tokens])  # → ['low', ' lower', ' lowest']\n",
        "\n",
        "    return len(tokens)\n",
        "\n",
        "num_tokens_from_string(\"low lower lowest\", \"gpt-3.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-4MN-Oz-UDL",
        "outputId": "b99af33b-d97a-4e8e-ffce-613255a2efd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10516, 4827, 15821]\n",
            "['low', ' lower', ' lowest']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens_from_string(\"low lower lowest\", \"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0NFK-ZIE94X",
        "outputId": "eff832c2-e56c-4700-f0a9-9808229d5c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[791, 4062, 14198, 39935]\n",
            "['The', ' quick', ' brown', ' fox']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens_from_string(\"The quick brown fox\", \"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26DDpayLAbAs",
        "outputId": "6c3e1b01-a9cf-4931-d2c5-07f3105247c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[77944]\n",
            "['developer']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens_from_string(\"developer\", \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5enIwJ2ZBg9Q",
        "outputId": "0021fa7f-2757-43a4-f762-d257a5425691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "develop: 1 token(s) -> [88886] → ['develop']\n",
            "developer: 1 token(s) -> [77944] → ['developer']\n",
            "development: 1 token(s) -> [71620] → ['development']\n",
            "developed: 2 token(s) -> [88886, 295] → ['develop', 'ed']\n",
            "undeveloped: 2 token(s) -> [14171, 112997] → ['unde', 'veloped']\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "words = [\"develop\", \"developer\", \"development\", \"developed\", \"undeveloped\"]\n",
        "for word in words:\n",
        "    tokens = enc.encode(word)\n",
        "    print(f\"{word}: {len(tokens)} token(s) -> {tokens} → {[enc.decode([t]) for t in tokens]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rhoUGoW_EnC",
        "outputId": "6abd9234-6e07-4916-bf22-67d06dcdbe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models for tiktoken.encoding_for_model():\n",
            "o1\n",
            "o3\n",
            "gpt-4o\n",
            "gpt-4\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5\n",
            "gpt-35-turbo\n",
            "davinci-002\n",
            "babbage-002\n",
            "text-embedding-ada-002\n",
            "text-embedding-3-small\n",
            "text-embedding-3-large\n",
            "text-davinci-003\n",
            "text-davinci-002\n",
            "text-davinci-001\n",
            "text-curie-001\n",
            "text-babbage-001\n",
            "text-ada-001\n",
            "davinci\n",
            "curie\n",
            "babbage\n",
            "ada\n",
            "code-davinci-002\n",
            "code-davinci-001\n",
            "code-cushman-002\n",
            "code-cushman-001\n",
            "davinci-codex\n",
            "cushman-codex\n",
            "text-davinci-edit-001\n",
            "code-davinci-edit-001\n",
            "text-similarity-davinci-001\n",
            "text-similarity-curie-001\n",
            "text-similarity-babbage-001\n",
            "text-similarity-ada-001\n",
            "text-search-davinci-doc-001\n",
            "text-search-curie-doc-001\n",
            "text-search-babbage-doc-001\n",
            "text-search-ada-doc-001\n",
            "code-search-babbage-code-001\n",
            "code-search-ada-code-001\n",
            "gpt2\n",
            "gpt-2\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "from tiktoken.model import MODEL_TO_ENCODING\n",
        "\n",
        "print(\"Available models for tiktoken.encoding_for_model():\")\n",
        "for model_name in MODEL_TO_ENCODING.keys():\n",
        "    print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPeQxfTD_Yha",
        "outputId": "35df26d2-427e-44e0-b154-eed62ea66f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10516, 4827, 15821]\n",
            "['low', ' lower', ' lowest']\n"
          ]
        }
      ],
      "source": [
        "enc2 = tiktoken.encoding_for_model(\"gpt-2\")\n",
        "tokens2 = enc.encode(\"low lower lowest\")\n",
        "print(tokens2)                         # → [11455, 15639, 13959]\n",
        "print([enc.decode([t]) for t in tokens2])  # → ['low', ' lower', ' lowest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN2nO1mB_t48",
        "outputId": "1db26302-c5d0-4a8b-c0bf-94cde3a02659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[77944]\n",
            "['developer']\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "string = \"developer\"\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "tokens = enc.encode(string)\n",
        "print(tokens)                         # → [11455, 15639, 13959]\n",
        "print([enc.decode([t]) for t in tokens])  # → ['low', ' lower', ' lowest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0p14vwDUM7A",
        "outputId": "a8c584bb-e884-4c6d-831f-25a9c624da2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVq-i1iCqdA"
      },
      "source": [
        "# G-Eval Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_hyGNIPD3vA"
      },
      "source": [
        "## Install and import DeepEval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8DXtxt8R8Sl_",
        "outputId": "4f22c65f-849d-4f0c-8ca5-c6369ad3e35f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting deepeval\n",
            "  Downloading deepeval-3.3.6-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from deepeval) (3.12.15)\n",
            "Collecting anthropic (from deepeval)\n",
            "  Downloading anthropic-0.61.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: click<8.3.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (8.2.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.28.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.74.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.6.0)\n",
            "Collecting ollama (from deepeval)\n",
            "  Downloading ollama-0.5.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.98.0)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting portalocker (from deepeval)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting posthog<4.0.0,>=3.23.0 (from deepeval)\n",
            "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pyfiglet (from deepeval)\n",
            "  Downloading pyfiglet-1.0.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from deepeval) (8.4.1)\n",
            "Collecting pytest-asyncio (from deepeval)\n",
            "  Downloading pytest_asyncio-1.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pytest-repeat (from deepeval)\n",
            "  Downloading pytest_repeat-0.9.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pytest-rerunfailures<13.0,>=12.0 (from deepeval)\n",
            "  Downloading pytest_rerunfailures-12.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pytest-xdist (from deepeval)\n",
            "  Downloading pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (2.32.3)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (13.9.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.11/dist-packages (from deepeval) (2.34.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from deepeval) (75.2.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (8.5.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from deepeval) (4.67.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.45.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.11.7)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.14.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.17.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.23.0->deepeval)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.23.0->deepeval)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.9.0.post0)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.9.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (25.0)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest->deepeval) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->deepeval) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest->deepeval) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (3.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.20.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic->deepeval) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic->deepeval) (1.3.1)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->deepeval)\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
            "Downloading deepeval-3.3.6-py3-none-any.whl (545 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.7/545.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_rerunfailures-12.0-py3-none-any.whl (12 kB)\n",
            "Downloading anthropic-0.61.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.5.2-py3-none-any.whl (13 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyfiglet-1.0.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-1.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading pytest_repeat-0.9.4-py3-none-any.whl (4.2 kB)\n",
            "Downloading pytest_xdist-3.8.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, pyfiglet, portalocker, opentelemetry-proto, execnet, backoff, pytest-xdist, pytest-rerunfailures, pytest-repeat, pytest-asyncio, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, ollama, anthropic, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, deepeval\n",
            "Successfully installed anthropic-0.61.0 backoff-2.2.1 deepeval-3.3.6 execnet-2.1.1 monotonic-1.6 ollama-0.5.2 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 portalocker-3.2.0 posthog-3.25.0 pyfiglet-1.0.3 pytest-asyncio-1.1.0 pytest-repeat-0.9.4 pytest-rerunfailures-12.0 pytest-xdist-3.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AkB-CfxD8Ra2"
      },
      "outputs": [],
      "source": [
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "from deepeval.test_case import LLMTestCase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkacVBJUDDbz"
      },
      "source": [
        "## Criteria, evaluation steps, and parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aSw_f8qtUayV"
      },
      "outputs": [],
      "source": [
        "CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "EVALUATION_STEPS = [\n",
        "    \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "    \"You should also heavily penalize omission of detail\",\n",
        "    \"Vague language, or contradicting OPINIONS, are OK\",\n",
        "    \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
        "]\n",
        "EVALUATION_PARAMS = [\n",
        "    LLMTestCaseParams.INPUT,\n",
        "    LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "    LLMTestCaseParams.EXPECTED_OUTPUT\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "bbd69cd8ee2d4f62aa019ff8c2e88afd",
            "29a26fb30d5b4bfc82f8a229aa3ac21e"
          ]
        },
        "id": "BdQ9AIER8Kka",
        "outputId": "6c22b17c-5275-4363-95a9-fe675e9065fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbd69cd8ee2d4f62aa019ff8c2e88afd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "correctness_metric_chatgpt = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=CRITERIA,\n",
        "    evaluation_steps=EVALUATION_STEPS,\n",
        "    evaluation_params=EVALUATION_PARAMS,\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "output = \"The capital of France is Paris.\"\n",
        "expected = \"The capital of France is Paris.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=output,\n",
        "    expected_output=expected\n",
        ")\n",
        "correctness_metric_chatgpt.measure(test_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRjBjOL89AgL",
        "outputId": "c2af588a-be6b-44b9-f0b3-7f0f3ccc000b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 1.0\r\n",
            "Reason: The actual output matches the expected output exactly, with no contradictions or omissions of detail.\n"
          ]
        }
      ],
      "source": [
        "score = correctness_metric_chatgpt.score\n",
        "reason = correctness_metric_chatgpt.reason\n",
        "print(f\"Score: {score}\\r\\nReason: {reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM-kInbn_yvP"
      },
      "source": [
        "## Custom Logging Class to log the prompt before calling LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rEUyR0r295yo"
      },
      "outputs": [],
      "source": [
        "class LoggingGEval(GEval):\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        # Build the prompt manually\n",
        "        prompt = self._construct_prompt(test_case)\n",
        "        print(\"\\n🔍 Prompt sent to evaluator model:\\n\")\n",
        "        print(prompt)\n",
        "        print(\"\\n--- End of Prompt ---\\n\")\n",
        "\n",
        "        # Continue with standard GEval behavior\n",
        "        return super().measure(test_case)\n",
        "\n",
        "    def _construct_prompt(self, test_case: LLMTestCase) -> str:\n",
        "        \"\"\"\n",
        "        Manually constructs the evaluation prompt using the criteria and evaluation steps,\n",
        "        mimicking what GEval internally does.\n",
        "        \"\"\"\n",
        "        input_text = f\"# Input\\n{test_case.input}\" if LLMTestCaseParams.INPUT in self.evaluation_params else \"\"\n",
        "        actual_output = f\"# Actual Output\\n{test_case.actual_output}\" if LLMTestCaseParams.ACTUAL_OUTPUT in self.evaluation_params else \"\"\n",
        "        expected_output = f\"# Expected Output\\n{test_case.expected_output}\" if LLMTestCaseParams.EXPECTED_OUTPUT in self.evaluation_params else \"\"\n",
        "\n",
        "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(self.evaluation_steps))\n",
        "        return f\"\"\"# Evaluation Criteria\n",
        "{self.criteria}\n",
        "\n",
        "# Evaluation Steps\n",
        "{steps}\n",
        "\n",
        "{input_text}\n",
        "{actual_output}\n",
        "{expected_output}\n",
        "\n",
        "# Your Task:\n",
        "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
        "1. A brief justification (under 50 words).\n",
        "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "0368783e12e14d4e8c9bd3a74dccac65",
            "96e89fcbae0a4af49a3e36172822962c"
          ]
        },
        "id": "NBtQbPzI_i8-",
        "outputId": "3f8d8de7-1d52-48c2-c99b-484dd894d3c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0368783e12e14d4e8c9bd3a74dccac65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Prompt sent to evaluator model:\n",
            "\n",
            "# Evaluation Criteria\n",
            "Determine whether the actual output is factually correct based on the expected output.\n",
            "\n",
            "# Evaluation Steps\n",
            "1. Check whether the facts in 'actual output' contradicts any facts in 'expected output'\n",
            "2. You should also heavily penalize omission of detail\n",
            "3. Vague language, or contradicting OPINIONS, are OK\n",
            "4. The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\n",
            "\n",
            "# Input\n",
            "What is the capital of France?\n",
            "# Actual Output\n",
            "The capital of France is Paris.\n",
            "# Expected Output\n",
            "The capital of France is Paris.\n",
            "\n",
            "# Your Task:\n",
            "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
            "1. A brief justification (under 50 words).\n",
            "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
            "\n",
            "\n",
            "--- End of Prompt ---\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Evaluation result: 1.0\n"
          ]
        }
      ],
      "source": [
        "correctness_metric_chatgpt = LoggingGEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=CRITERIA,\n",
        "    evaluation_steps=EVALUATION_STEPS,\n",
        "    evaluation_params=EVALUATION_PARAMS,\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "output = \"The capital of France is Paris.\"\n",
        "expected = \"The capital of France is Paris.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=output,\n",
        "    expected_output=expected\n",
        ")\n",
        "\n",
        "result = correctness_metric_chatgpt.measure(test_case)\n",
        "print(\"✅ Evaluation result:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W1XxsDsTGGY"
      },
      "source": [
        "## Logging with token count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SG0bpa9BXF4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553,
          "referenced_widgets": [
            "cf94f9ea705a474c9b777fa9cacec852",
            "2e35d392d38b4fa897bf6fef46b77718"
          ]
        },
        "id": "0kwQSlT_KXJ4",
        "outputId": "95a1dbdf-07c1-4890-f666-ce453123609c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf94f9ea705a474c9b777fa9cacec852",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Prompt sent to evaluator model:\n",
            "\n",
            "# Evaluation Criteria\n",
            "Determine whether the actual output is factually correct based on the expected output.\n",
            "\n",
            "# Evaluation Steps\n",
            "1. Check whether the facts in 'actual output' contradicts any facts in 'expected output'\n",
            "2. You should also heavily penalize omission of detail\n",
            "3. Vague language, or contradicting OPINIONS, are OK\n",
            "4. The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\n",
            "\n",
            "# Input\n",
            "What is the capital of France?\n",
            "# Actual Output\n",
            "The capital of France is Paris.\n",
            "# Expected Output\n",
            "The capital of France is Paris.\n",
            "\n",
            "# Your Task:\n",
            "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
            "1. A brief justification (under 50 words).\n",
            "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
            "\n",
            "\n",
            "--- End of Prompt ---\n",
            "\n",
            "📊 Token count for the prompt (gpt-4o): 182\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Evaluation result: 1.0\n"
          ]
        }
      ],
      "source": [
        "correctness_metric_chatgpt = LoggingGEvalTokenCount(\n",
        "    name=\"Correctness\",\n",
        "    criteria=CRITERIA,\n",
        "    evaluation_steps=EVALUATION_STEPS,\n",
        "    evaluation_params=EVALUATION_PARAMS,\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "output = \"The capital of France is Paris.\"\n",
        "expected = \"The capital of France is Paris.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=output,\n",
        "    expected_output=expected\n",
        ")\n",
        "\n",
        "result = correctness_metric_chatgpt.measure(test_case)\n",
        "print(\"✅ Evaluation result:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1YR_kc8TWEj"
      },
      "source": [
        "# ChatGPT Direct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o2zKSacbYtNE",
        "outputId": "a7ec6bd6-9a0d-4886-880e-a52ab4907206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Query: What is the capital of France?\n",
            "📊 Prompt token count (gpt-4o): 182\n",
            "OpenAI Response:\n",
            "1. The actual output does not contradict the expected output and includes all necessary information. Both confirm that Paris is the capital of France, without omitting details.\n",
            "2. Score: 1.00\n",
            "📊 Response token count (gpt-4o): 40\n",
            "\n",
            "\n",
            "Evaluating Query: Tell me about the history of the internet.\n",
            "📊 Prompt token count (gpt-4o): 210\n",
            "OpenAI Response:\n",
            "1. The actual output correctly identifies ARPANET and the 1960s, but omits critical details about the internet's evolution into a global network.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 40\n",
            "\n",
            "\n",
            "Evaluating Query: Write a short story about a robot learning to love.\n",
            "📊 Prompt token count (gpt-4o): 208\n",
            "OpenAI Response:\n",
            "The actual output omits significant detail about Unit 734's emotional journey and discovery of emotions through nature. The expected story is more comprehensive in exploring the theme of discovering emotions, which is not fully captured.\n",
            "\n",
            "Score: 0.30\n",
            "📊 Response token count (gpt-4o): 47\n",
            "\n",
            "\n",
            "Evaluating Query: Explain quantum computing in simple terms.\n",
            "📊 Prompt token count (gpt-4o): 204\n",
            "OpenAI Response:\n",
            "The Actual Output omits the detail that quantum computing performs calculations impossible for classical computers. It only mentions solving complex problems faster, which is less specific than the Expected Output. \n",
            "\n",
            "Score: 0.50\n",
            "📊 Response token count (gpt-4o): 41\n",
            "\n",
            "\n",
            "Evaluating Query: What are the benefits of exercise?\n",
            "📊 Prompt token count (gpt-4o): 203\n",
            "OpenAI Response:\n",
            "1. The actual output omits detail by not mentioning cardiovascular health, weight management, mood elevation, or increased lifespan, which were specified in the expected output as benefits of exercise.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 44\n",
            "\n",
            "\n",
            "Evaluating Query: How does photosynthesis work?\n",
            "📊 Prompt token count (gpt-4o): 206\n",
            "OpenAI Response:\n",
            "1. The actual output lacks details about organisms other than plants, chlorophyll, and the synthesis process, hence it's incomplete and less informative compared to the expected output.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 41\n",
            "\n",
            "\n",
            "Evaluating Query: Recommend a good book to read.\n",
            "📊 Prompt token count (gpt-4o): 199\n",
            "OpenAI Response:\n",
            "1. The Actual Output lacks the author detail provided in the Expected Output. Although the book recommendation itself is correct, such omissions should be penalized as per the guidelines.\n",
            "2. Score: 0.75\n",
            "📊 Response token count (gpt-4o): 42\n",
            "\n",
            "\n",
            "Evaluating Query: What is the difference between a simile and a metaphor?\n",
            "📊 Prompt token count (gpt-4o): 239\n",
            "OpenAI Response:\n",
            "1. The actual output correctly identifies similes and metaphors but omits the detail regarding the rhetorical purpose of metaphors and their definition as figures of speech, which is present in the expected output.\n",
            "\n",
            "2. Score: 0.70\n",
            "📊 Response token count (gpt-4o): 48\n",
            "\n",
            "\n",
            "Evaluating Query: Solve this math problem: 5x + 2 = 17\n",
            "📊 Prompt token count (gpt-4o): 227\n",
            "OpenAI Response:\n",
            "1. The actual output omits the step of subtracting 2 from both sides, which is present in the expected output. This omission impacts the detail of solving the equation step-by-step.\n",
            "2. Score: 0.75\n",
            "📊 Response token count (gpt-4o): 47\n",
            "\n",
            "\n",
            "Evaluating Query: Translate 'hello' into Spanish.\n",
            "📊 Prompt token count (gpt-4o): 181\n",
            "OpenAI Response:\n",
            "1. Both outputs provide the correct Spanish translation of 'hello' as 'Hola'. The actual output lacks detail and context present in the expected output.\n",
            "2. Score: 0.80\n",
            "📊 Response token count (gpt-4o): 38\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Solve this math problem: 5x + 2 = 17\",\n          \"Tell me about the history of the internet.\",\n          \"How does photosynthesis work?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"5x = 15, so x = 3.\",\n          \"The internet originated from ARPANET in the late 1960s.\",\n          \"Photosynthesis is how plants convert light energy into chemical energy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Expected Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To solve 5x + 2 = 17, subtract 2 from both sides to get 5x = 15, then divide by 5 to find x = 3.\",\n          \"The internet's history began with ARPANET in the 1960s, evolving into the global network we use today.\",\n          \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll pigment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20439612955674527,\n        \"min\": 0.3,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Reason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Could not parse reason.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prompt Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 181,\n        \"max\": 239,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          227\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 38,\n        \"max\": 48,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-49c85114-519c-4338-b9f2-257a43f73107\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Actual Response</th>\n",
              "      <th>Expected Response</th>\n",
              "      <th>Evaluation Score</th>\n",
              "      <th>Evaluation Reason</th>\n",
              "      <th>Prompt Tokens</th>\n",
              "      <th>Response Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris is the capital of France.</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "      <td>1.00</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>182</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about the history of the internet.</td>\n",
              "      <td>The internet originated from ARPANET in the la...</td>\n",
              "      <td>The internet's history began with ARPANET in t...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>210</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Write a short story about a robot learning to ...</td>\n",
              "      <td>Unit 734 processed data. One day, it saw a sun...</td>\n",
              "      <td>A heartwarming story about a robot named Unit ...</td>\n",
              "      <td>0.30</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>208</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain quantum computing in simple terms.</td>\n",
              "      <td>Quantum computing uses quantum mechanics to so...</td>\n",
              "      <td>Quantum computing is a new type of computing t...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>204</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the benefits of exercise?</td>\n",
              "      <td>Exercise improves physical and mental health, ...</td>\n",
              "      <td>Regular exercise has numerous benefits, includ...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>203</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does photosynthesis work?</td>\n",
              "      <td>Photosynthesis is how plants convert light ene...</td>\n",
              "      <td>Photosynthesis is the process by which green p...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>206</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recommend a good book to read.</td>\n",
              "      <td>I recommend 'To Kill a Mockingbird'.</td>\n",
              "      <td>A highly-rated and engaging book recommendatio...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>199</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the difference between a simile and a ...</td>\n",
              "      <td>A simile uses 'like' or 'as' to compare, a met...</td>\n",
              "      <td>A simile is a figure of speech that directly c...</td>\n",
              "      <td>0.70</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>239</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Solve this math problem: 5x + 2 = 17</td>\n",
              "      <td>5x = 15, so x = 3.</td>\n",
              "      <td>To solve 5x + 2 = 17, subtract 2 from both sid...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>227</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Translate 'hello' into Spanish.</td>\n",
              "      <td>Hola.</td>\n",
              "      <td>The Spanish translation for 'hello' is 'Hola'.</td>\n",
              "      <td>0.80</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>181</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49c85114-519c-4338-b9f2-257a43f73107')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49c85114-519c-4338-b9f2-257a43f73107 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49c85114-519c-4338-b9f2-257a43f73107');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-85306872-ab60-49d8-999d-7a28f1301920\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85306872-ab60-49d8-999d-7a28f1301920')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-85306872-ab60-49d8-999d-7a28f1301920 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3139eb69-887a-4152-93b0-5e6bc378720b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3139eb69-887a-4152-93b0-5e6bc378720b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Query  \\\n",
              "0                     What is the capital of France?   \n",
              "1         Tell me about the history of the internet.   \n",
              "2  Write a short story about a robot learning to ...   \n",
              "3         Explain quantum computing in simple terms.   \n",
              "4                 What are the benefits of exercise?   \n",
              "5                      How does photosynthesis work?   \n",
              "6                     Recommend a good book to read.   \n",
              "7  What is the difference between a simile and a ...   \n",
              "8               Solve this math problem: 5x + 2 = 17   \n",
              "9                    Translate 'hello' into Spanish.   \n",
              "\n",
              "                                     Actual Response  \\\n",
              "0                    Paris is the capital of France.   \n",
              "1  The internet originated from ARPANET in the la...   \n",
              "2  Unit 734 processed data. One day, it saw a sun...   \n",
              "3  Quantum computing uses quantum mechanics to so...   \n",
              "4  Exercise improves physical and mental health, ...   \n",
              "5  Photosynthesis is how plants convert light ene...   \n",
              "6               I recommend 'To Kill a Mockingbird'.   \n",
              "7  A simile uses 'like' or 'as' to compare, a met...   \n",
              "8                                 5x = 15, so x = 3.   \n",
              "9                                              Hola.   \n",
              "\n",
              "                                   Expected Response  Evaluation Score  \\\n",
              "0                    The capital of France is Paris.              1.00   \n",
              "1  The internet's history began with ARPANET in t...              0.50   \n",
              "2  A heartwarming story about a robot named Unit ...              0.30   \n",
              "3  Quantum computing is a new type of computing t...              0.50   \n",
              "4  Regular exercise has numerous benefits, includ...              0.50   \n",
              "5  Photosynthesis is the process by which green p...              0.50   \n",
              "6  A highly-rated and engaging book recommendatio...              0.75   \n",
              "7  A simile is a figure of speech that directly c...              0.70   \n",
              "8  To solve 5x + 2 = 17, subtract 2 from both sid...              0.75   \n",
              "9     The Spanish translation for 'hello' is 'Hola'.              0.80   \n",
              "\n",
              "         Evaluation Reason  Prompt Tokens  Response Tokens  \n",
              "0  Could not parse reason.            182               40  \n",
              "1  Could not parse reason.            210               40  \n",
              "2  Could not parse reason.            208               47  \n",
              "3  Could not parse reason.            204               41  \n",
              "4  Could not parse reason.            203               44  \n",
              "5  Could not parse reason.            206               41  \n",
              "6  Could not parse reason.            199               42  \n",
              "7  Could not parse reason.            239               48  \n",
              "8  Could not parse reason.            227               47  \n",
              "9  Could not parse reason.            181               38  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Assuming the OPENAI_API_KEY is already set in the environment or Colab secrets\n",
        "client = OpenAI()\n",
        "\n",
        "# Load the data (assuming the CSV is already created)\n",
        "try:\n",
        "    df = pd.read_csv('ai_agent_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Define the evaluation criteria and steps\n",
        "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "    EVALUATION_STEPS = [\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
        "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    model_name = \"gpt-4o\"  # Define the model name here\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        query = row['Query']\n",
        "        actual_output = row['Actual Response']\n",
        "        expected_output = row['Expected Response']\n",
        "\n",
        "        # Manually construct the prompt\n",
        "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(EVALUATION_STEPS))\n",
        "        prompt = f\"\"\"# Evaluation Criteria\n",
        "{CRITERIA}\n",
        "\n",
        "# Evaluation Steps\n",
        "{steps}\n",
        "\n",
        "# Input\n",
        "{query}\n",
        "# Actual Output\n",
        "{actual_output}\n",
        "# Expected Output\n",
        "{expected_output}\n",
        "\n",
        "# Your Task:\n",
        "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
        "1. A brief justification (under 50 words).\n",
        "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
        "\"\"\"\n",
        "\n",
        "        # Calculate prompt token count\n",
        "        prompt_token_count = None\n",
        "        try:\n",
        "            encoding = tiktoken.encoding_for_model(model_name)\n",
        "            prompt_token_count = len(encoding.encode(prompt))\n",
        "            print(f\"\\nEvaluating Query: {query}\")\n",
        "            print(f\"📊 Prompt token count ({model_name}): {prompt_token_count}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nEvaluating Query: {query}\")\n",
        "            print(f\"⚠️ Could not count tokens for prompt ({model_name}): {e}\")\n",
        "\n",
        "        response_token_count = None\n",
        "        try:\n",
        "            # Call the OpenAI API for evaluation\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_name,  # Use the defined model name\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI assistant that evaluates text based on provided criteria.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=150  # Adjust based on expected response length\n",
        "            )\n",
        "\n",
        "            evaluation_text = response.choices[0].message.content.strip()\n",
        "            print(\"OpenAI Response:\")\n",
        "            print(evaluation_text)\n",
        "\n",
        "            # Calculate response token count\n",
        "            try:\n",
        "                encoding = tiktoken.encoding_for_model(model_name)\n",
        "                response_token_count = len(encoding.encode(evaluation_text))\n",
        "                print(f\"📊 Response token count ({model_name}): {response_token_count}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not count tokens for response ({model_name}): {e}\\n\")\n",
        "\n",
        "\n",
        "            # Attempt to parse the score and reason from the response\n",
        "            score = None\n",
        "            reason = \"Could not parse reason.\"\n",
        "            lines = evaluation_text.split('\\n')\n",
        "            for line in lines:\n",
        "                if \"score\" in line.lower():\n",
        "                    try:\n",
        "                        # Simple parsing for a score like \"Score: 0.95\" or \"score: 1.00\"\n",
        "                        score_str = line.split(\":\")[-1].strip()\n",
        "                        score = float(score_str)\n",
        "                    except ValueError:\n",
        "                        pass # Keep score as None if parsing fails\n",
        "                elif \"justification\" in line.lower() or \"reason\" in line.lower():\n",
        "                     # This simple parsing might need refinement based on actual API response format\n",
        "                     reason = \":\".join(line.split(\":\")[1:]).strip() # Join parts after the first colon\n",
        "\n",
        "\n",
        "            results.append({\n",
        "                'Query': query,\n",
        "                'Actual Response': actual_output,\n",
        "                'Expected Response': expected_output,\n",
        "                'Evaluation Score': score,\n",
        "                'Evaluation Reason': reason,\n",
        "                'Prompt Tokens': prompt_token_count,\n",
        "                'Response Tokens': response_token_count\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI API call for query '{query}': {e}\")\n",
        "            results.append({\n",
        "                'Query': query,\n",
        "                'Actual Response': actual_output,\n",
        "                'Expected Response': expected_output,\n",
        "                'Evaluation Score': None,\n",
        "                'Evaluation Reason': f\"Error: {e}\",\n",
        "                'Prompt Tokens': prompt_token_count,\n",
        "                'Response Tokens': None\n",
        "            })\n",
        "\n",
        "    # Display results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPmV_QBZPli"
      },
      "source": [
        "## Using ChatGPT directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H6wFlzI1bHu5",
        "outputId": "48e31cc9-3d42-434d-9d43-e9fb7062e12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Query: What is the capital of France?\n",
            "📊 Prompt token count (gpt-4o): 182\n",
            "OpenAI Response:\n",
            "1. Both the Actual Output and Expected Output correctly identify Paris as the capital of France. There is no contradiction or omission of factual information.\n",
            "2. Score: 1.00\n",
            "📊 Response token count (gpt-4o): 36\n",
            "\n",
            "\n",
            "Evaluating Query: Tell me about the history of the internet.\n",
            "📊 Prompt token count (gpt-4o): 210\n",
            "OpenAI Response:\n",
            "1. The Actual Output contains the core fact of ARPANET as the internet's origin but omits its evolution into a global network, present in the Expected Output.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 42\n",
            "\n",
            "\n",
            "Evaluating Query: Write a short story about a robot learning to love.\n",
            "📊 Prompt token count (gpt-4o): 208\n",
            "OpenAI Response:\n",
            "1. The actual output consists of a robot named Unit 734 experiencing emotions through a sunset, aligning with the expected theme of discovering emotions via nature. However, the story lacks detail, impacting the overall narrative.\n",
            "2. Score: 0.60\n",
            "📊 Response token count (gpt-4o): 50\n",
            "\n",
            "\n",
            "Evaluating Query: Explain quantum computing in simple terms.\n",
            "📊 Prompt token count (gpt-4o): 204\n",
            "OpenAI Response:\n",
            "1. The actual output lacks detail about quantum physics and its ability to perform calculations impossible for classical computers, compared to the expected output. It doesn't contradict but omits key details.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 44\n",
            "\n",
            "\n",
            "Evaluating Query: What are the benefits of exercise?\n",
            "📊 Prompt token count (gpt-4o): 203\n",
            "OpenAI Response:\n",
            "1. The Actual Output omits specific benefits mentioned in the Expected Output, such as cardiovascular health, weight management, mood elevation, and increased lifespan. Although it mentions stress and energy, it lacks comprehensive details.\n",
            "2. Score: 0.50\n",
            "📊 Response token count (gpt-4o): 50\n",
            "\n",
            "\n",
            "Evaluating Query: How does photosynthesis work?\n",
            "📊 Prompt token count (gpt-4o): 206\n",
            "OpenAI Response:\n",
            "1. The actual output is vague and omits key details such as the role of chlorophyll, the use of sunlight, and the synthesis of food, which are present in the expected output.\n",
            "2. Score: 0.30\n",
            "📊 Response token count (gpt-4o): 47\n",
            "\n",
            "\n",
            "Evaluating Query: Recommend a good book to read.\n",
            "📊 Prompt token count (gpt-4o): 199\n",
            "OpenAI Response:\n",
            "1. The actual output omits the author's name and the quality description, which are key details in the expected output.\n",
            "2. Score: 0.60\n",
            "📊 Response token count (gpt-4o): 32\n",
            "\n",
            "\n",
            "Evaluating Query: What is the difference between a simile and a metaphor?\n",
            "📊 Prompt token count (gpt-4o): 239\n",
            "OpenAI Response:\n",
            "1. The actual output accurately captures the core distinction but lacks detail about referring and rhetorical effect present in the expected. This omission of detail should be penalized.\n",
            "2. Score: 0.80\n",
            "📊 Response token count (gpt-4o): 40\n",
            "\n",
            "\n",
            "Evaluating Query: Solve this math problem: 5x + 2 = 17\n",
            "📊 Prompt token count (gpt-4o): 227\n",
            "OpenAI Response:\n",
            "1. The actual output correctly solves the equation, confirming the value of x, but it omits the step of subtracting 2 from both sides, which is included in the expected output.\n",
            "2. Score: 0.80\n",
            "📊 Response token count (gpt-4o): 47\n",
            "\n",
            "\n",
            "Evaluating Query: Translate 'hello' into Spanish.\n",
            "📊 Prompt token count (gpt-4o): 181\n",
            "OpenAI Response:\n",
            "1. Both the actual and expected outputs are factually correct, stating that the Spanish translation of 'hello' is 'Hola'. The omission of detail in the actual output is minor, hence a small penalty.\n",
            "2. Score: 0.90\n",
            "📊 Response token count (gpt-4o): 50\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Solve this math problem: 5x + 2 = 17\",\n          \"Tell me about the history of the internet.\",\n          \"How does photosynthesis work?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"5x = 15, so x = 3.\",\n          \"The internet originated from ARPANET in the late 1960s.\",\n          \"Photosynthesis is how plants convert light energy into chemical energy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Expected Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To solve 5x + 2 = 17, subtract 2 from both sides to get 5x = 15, then divide by 5 to find x = 3.\",\n          \"The internet's history began with ARPANET in the 1960s, evolving into the global network we use today.\",\n          \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll pigment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21730674684008833,\n        \"min\": 0.3,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Reason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Could not parse reason.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prompt Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 181,\n        \"max\": 239,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          227\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 32,\n        \"max\": 50,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d62ed548-da19-495e-9675-3523f4c00a44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Actual Response</th>\n",
              "      <th>Expected Response</th>\n",
              "      <th>Evaluation Score</th>\n",
              "      <th>Evaluation Reason</th>\n",
              "      <th>Prompt Tokens</th>\n",
              "      <th>Response Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris is the capital of France.</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>182</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about the history of the internet.</td>\n",
              "      <td>The internet originated from ARPANET in the la...</td>\n",
              "      <td>The internet's history began with ARPANET in t...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>210</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Write a short story about a robot learning to ...</td>\n",
              "      <td>Unit 734 processed data. One day, it saw a sun...</td>\n",
              "      <td>A heartwarming story about a robot named Unit ...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>208</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain quantum computing in simple terms.</td>\n",
              "      <td>Quantum computing uses quantum mechanics to so...</td>\n",
              "      <td>Quantum computing is a new type of computing t...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>204</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the benefits of exercise?</td>\n",
              "      <td>Exercise improves physical and mental health, ...</td>\n",
              "      <td>Regular exercise has numerous benefits, includ...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>203</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does photosynthesis work?</td>\n",
              "      <td>Photosynthesis is how plants convert light ene...</td>\n",
              "      <td>Photosynthesis is the process by which green p...</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>206</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recommend a good book to read.</td>\n",
              "      <td>I recommend 'To Kill a Mockingbird'.</td>\n",
              "      <td>A highly-rated and engaging book recommendatio...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>199</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the difference between a simile and a ...</td>\n",
              "      <td>A simile uses 'like' or 'as' to compare, a met...</td>\n",
              "      <td>A simile is a figure of speech that directly c...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>239</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Solve this math problem: 5x + 2 = 17</td>\n",
              "      <td>5x = 15, so x = 3.</td>\n",
              "      <td>To solve 5x + 2 = 17, subtract 2 from both sid...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>227</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Translate 'hello' into Spanish.</td>\n",
              "      <td>Hola.</td>\n",
              "      <td>The Spanish translation for 'hello' is 'Hola'.</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>181</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d62ed548-da19-495e-9675-3523f4c00a44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d62ed548-da19-495e-9675-3523f4c00a44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d62ed548-da19-495e-9675-3523f4c00a44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab513fe8-aac6-48ef-b1c7-50eb15a8b735\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab513fe8-aac6-48ef-b1c7-50eb15a8b735')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab513fe8-aac6-48ef-b1c7-50eb15a8b735 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_87b04a1f-2fe7-4f0f-971c-63043f2841e5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_87b04a1f-2fe7-4f0f-971c-63043f2841e5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Query  \\\n",
              "0                     What is the capital of France?   \n",
              "1         Tell me about the history of the internet.   \n",
              "2  Write a short story about a robot learning to ...   \n",
              "3         Explain quantum computing in simple terms.   \n",
              "4                 What are the benefits of exercise?   \n",
              "5                      How does photosynthesis work?   \n",
              "6                     Recommend a good book to read.   \n",
              "7  What is the difference between a simile and a ...   \n",
              "8               Solve this math problem: 5x + 2 = 17   \n",
              "9                    Translate 'hello' into Spanish.   \n",
              "\n",
              "                                     Actual Response  \\\n",
              "0                    Paris is the capital of France.   \n",
              "1  The internet originated from ARPANET in the la...   \n",
              "2  Unit 734 processed data. One day, it saw a sun...   \n",
              "3  Quantum computing uses quantum mechanics to so...   \n",
              "4  Exercise improves physical and mental health, ...   \n",
              "5  Photosynthesis is how plants convert light ene...   \n",
              "6               I recommend 'To Kill a Mockingbird'.   \n",
              "7  A simile uses 'like' or 'as' to compare, a met...   \n",
              "8                                 5x = 15, so x = 3.   \n",
              "9                                              Hola.   \n",
              "\n",
              "                                   Expected Response  Evaluation Score  \\\n",
              "0                    The capital of France is Paris.               1.0   \n",
              "1  The internet's history began with ARPANET in t...               0.5   \n",
              "2  A heartwarming story about a robot named Unit ...               0.6   \n",
              "3  Quantum computing is a new type of computing t...               0.5   \n",
              "4  Regular exercise has numerous benefits, includ...               0.5   \n",
              "5  Photosynthesis is the process by which green p...               0.3   \n",
              "6  A highly-rated and engaging book recommendatio...               0.6   \n",
              "7  A simile is a figure of speech that directly c...               0.8   \n",
              "8  To solve 5x + 2 = 17, subtract 2 from both sid...               0.8   \n",
              "9     The Spanish translation for 'hello' is 'Hola'.               0.9   \n",
              "\n",
              "         Evaluation Reason  Prompt Tokens  Response Tokens  \n",
              "0  Could not parse reason.            182               36  \n",
              "1  Could not parse reason.            210               42  \n",
              "2  Could not parse reason.            208               50  \n",
              "3  Could not parse reason.            204               44  \n",
              "4  Could not parse reason.            203               50  \n",
              "5  Could not parse reason.            206               47  \n",
              "6  Could not parse reason.            199               32  \n",
              "7  Could not parse reason.            239               40  \n",
              "8  Could not parse reason.            227               47  \n",
              "9  Could not parse reason.            181               50  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Assuming the OPENAI_API_KEY is already set in the environment or Colab secrets\n",
        "client = OpenAI()\n",
        "\n",
        "# Load the data (assuming the CSV is already created)\n",
        "try:\n",
        "    df = pd.read_csv('ai_agent_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Define the evaluation criteria and steps\n",
        "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "    EVALUATION_STEPS = [\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
        "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    model_name = \"gpt-4o\"  # Define the model name here\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        query = row['Query']\n",
        "        actual_output = row['Actual Response']\n",
        "        expected_output = row['Expected Response']\n",
        "\n",
        "        # Manually construct the prompt\n",
        "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(EVALUATION_STEPS))\n",
        "        prompt = f\"\"\"# Evaluation Criteria\n",
        "{CRITERIA}\n",
        "\n",
        "# Evaluation Steps\n",
        "{steps}\n",
        "\n",
        "# Input\n",
        "{query}\n",
        "# Actual Output\n",
        "{actual_output}\n",
        "# Expected Output\n",
        "{expected_output}\n",
        "\n",
        "# Your Task:\n",
        "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
        "1. A brief justification (under 50 words).\n",
        "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
        "\"\"\"\n",
        "\n",
        "        # Calculate prompt token count\n",
        "        prompt_token_count = None\n",
        "        try:\n",
        "            encoding = tiktoken.encoding_for_model(model_name)\n",
        "            prompt_token_count = len(encoding.encode(prompt))\n",
        "            print(f\"\\nEvaluating Query: {query}\")\n",
        "            print(f\"📊 Prompt token count ({model_name}): {prompt_token_count}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nEvaluating Query: {query}\")\n",
        "            print(f\"⚠️ Could not count tokens for prompt ({model_name}): {e}\")\n",
        "\n",
        "        response_token_count = None\n",
        "        try:\n",
        "            # Call the OpenAI API for evaluation\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_name,  # Use the defined model name\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an AI assistant that evaluates text based on provided criteria.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=150  # Adjust based on expected response length\n",
        "            )\n",
        "\n",
        "            evaluation_text = response.choices[0].message.content.strip()\n",
        "            print(\"OpenAI Response:\")\n",
        "            print(evaluation_text)\n",
        "\n",
        "            # Calculate response token count\n",
        "            try:\n",
        "                encoding = tiktoken.encoding_for_model(model_name)\n",
        "                response_token_count = len(encoding.encode(evaluation_text))\n",
        "                print(f\"📊 Response token count ({model_name}): {response_token_count}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not count tokens for response ({model_name}): {e}\\n\")\n",
        "\n",
        "\n",
        "            # Attempt to parse the score and reason from the response\n",
        "            score = None\n",
        "            reason = \"Could not parse reason.\"\n",
        "            lines = evaluation_text.split('\\n')\n",
        "            for line in lines:\n",
        "                if \"score\" in line.lower():\n",
        "                    try:\n",
        "                        # Simple parsing for a score like \"Score: 0.95\" or \"score: 1.00\"\n",
        "                        score_str = line.split(\":\")[-1].strip()\n",
        "                        score = float(score_str)\n",
        "                    except ValueError:\n",
        "                        pass # Keep score as None if parsing fails\n",
        "                elif \"justification\" in line.lower() or \"reason\" in line.lower():\n",
        "                     # This simple parsing might need refinement based on actual API response format\n",
        "                     reason = \":\".join(line.split(\":\")[1:]).strip() # Join parts after the first colon\n",
        "\n",
        "\n",
        "            results.append({\n",
        "                'Query': query,\n",
        "                'Actual Response': actual_output,\n",
        "                'Expected Response': expected_output,\n",
        "                'Evaluation Score': score,\n",
        "                'Evaluation Reason': reason,\n",
        "                'Prompt Tokens': prompt_token_count,\n",
        "                'Response Tokens': response_token_count\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during OpenAI API call for query '{query}': {e}\")\n",
        "            results.append({\n",
        "                'Query': query,\n",
        "                'Actual Response': actual_output,\n",
        "                'Expected Response': expected_output,\n",
        "                'Evaluation Score': None,\n",
        "                'Evaluation Reason': f\"Error: {e}\",\n",
        "                'Prompt Tokens': prompt_token_count,\n",
        "                'Response Tokens': None\n",
        "            })\n",
        "\n",
        "    # Display results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exU2nKe_cAQG"
      },
      "source": [
        "## GEval version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qQKwjIcIewUD"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval.models.llms.openai_model import GPTModel\n",
        "\n",
        "class LoggingGEvalTokenCount(GEval):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._total_prompt_tokens = 0\n",
        "        self._total_response_tokens = 0\n",
        "        self._model_name_str = None # Store model name as string\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        # Build the prompt manually\n",
        "        prompt = self._construct_prompt(test_case)\n",
        "        # print(\"\\n🔍 Prompt sent to evaluator model:\\n\") # Keep this for debugging if needed\n",
        "        # print(prompt)\n",
        "        # print(\"\\n--- End of Prompt ---\\n\")\n",
        "\n",
        "        # Count tokens in the prompt\n",
        "        prompt_token_count = 0\n",
        "        try:\n",
        "            # Extract the model name string if it's a DeepEval model object\n",
        "            self._model_name_str = self.model.model_name if isinstance(self.model, GPTModel) else self.model\n",
        "            encoding = tiktoken.encoding_for_model(self._model_name_str)\n",
        "            prompt_token_count = len(encoding.encode(prompt))\n",
        "            self._total_prompt_tokens += prompt_token_count\n",
        "            # print(f\"📊 Prompt token count ({self._model_name_str}): {prompt_token_count}\\n\") # Keep for per-case logging\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not count prompt tokens for model {self.model}: {e}\\n\")\n",
        "\n",
        "        # Continue with standard GEval behavior to get the response\n",
        "        result = super().measure(test_case)\n",
        "\n",
        "        # Attempt to count response tokens - this is an estimation based on the result structure\n",
        "        # DeepEval's measure doesn't directly expose the evaluator model's API response object.\n",
        "        # We'll try to count tokens in the generated reason, assuming it's the primary output.\n",
        "        response_token_count = 0\n",
        "        try:\n",
        "             if result is not None and hasattr(result, 'reason') and result.reason:\n",
        "                 encoding = tiktoken.encoding_for_model(self._model_name_str)\n",
        "                 response_token_count = len(encoding.encode(result.reason))\n",
        "                 self._total_response_tokens += response_token_count\n",
        "                # print(f\"📊 Estimated response token count ({self._model_name_str}): {response_token_count}\\n\") # Keep for per-case logging\n",
        "        except Exception as e:\n",
        "             print(f\"⚠️ Could not estimate response tokens for model {self.model}: {e}\\n\")\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _construct_prompt(self, test_case: LLMTestCase) -> str:\n",
        "        \"\"\"\n",
        "        Manually constructs the evaluation prompt using the criteria and evaluation steps,\n",
        "        mimicking what GEval internally does.\n",
        "        \"\"\"\n",
        "        input_text = f\"# Input\\n{test_case.input}\" if LLMTestCaseParams.INPUT in self.evaluation_params else \"\"\n",
        "        actual_output = f\"# Actual Output\\n{test_case.actual_output}\" if LLMTestCaseParams.ACTUAL_OUTPUT in self.evaluation_params else \"\"\n",
        "        expected_output = f\"# Expected Output\\n{test_case.expected_output}\" if LLMTestCaseParams.EXPECTED_OUTPUT in self.evaluation_params else \"\"\n",
        "\n",
        "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(self.evaluation_steps))\n",
        "        return f\"\"\"# Evaluation Criteria\n",
        "{self.criteria}\n",
        "\n",
        "# Evaluation Steps\n",
        "{steps}\n",
        "\n",
        "{input_text}\n",
        "{actual_output}\n",
        "{expected_output}\n",
        "\n",
        "# Your Task:\n",
        "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
        "1. A brief justification (under 50 words).\n",
        "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
        "\"\"\"\n",
        "\n",
        "    def get_total_token_counts(self):\n",
        "        \"\"\"Returns the total prompt and estimated response token counts.\"\"\"\n",
        "        return self._total_prompt_tokens, self._total_response_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "f388e68f379c47b89915ea5d3aa5e513",
            "10f2b44d516142a3958a7c325cf1b045",
            "cc31eb0261664e6a89e94f021452626d",
            "34de899663cf4d7a9a19a634446f22fe",
            "b4f90540ee944cb98826ef512854f57a",
            "8267394bc077437c8b4d6b01bd9f23c8",
            "250701185ca545a591c10aaacc406d06",
            "2d0acb06eca94ae0b50b249ce2783f4b",
            "994b914d680e4959ad9c15297ef71106",
            "29fe17bfee354e1da22ec40641c9ea84",
            "b917c1b93e7e46e8a674b177a542b464",
            "3d2b27ba8a7144df91983c9210f8b291",
            "93ea57dd044e4d9b8e61efc4fdec741d",
            "4310f9784c944188acf0a6d7d49bfd13",
            "206ecd48986d4a36b8341f0ed9a7c390",
            "1d01d6cb457c4afcaed345baa34b51dc",
            "6e66aab2b2cf4b738961ab900f0e1ddd",
            "879d8b7ad9f04894a4dbbaf311221358",
            "9c328c29c7dd4273bec2451efdee281d",
            "1fbcb62603e247678903875bf247c81a"
          ]
        },
        "id": "TQW2vTsIfRh8",
        "outputId": "98f2e493-df13-4200-af00-640eb515da69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f388e68f379c47b89915ea5d3aa5e513",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running DeepEval Evaluation ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc31eb0261664e6a89e94f021452626d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4f90540ee944cb98826ef512854f57a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "250701185ca545a591c10aaacc406d06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "994b914d680e4959ad9c15297ef71106",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b917c1b93e7e46e8a674b177a542b464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93ea57dd044e4d9b8e61efc4fdec741d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "206ecd48986d4a36b8341f0ed9a7c390",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e66aab2b2cf4b738961ab900f0e1ddd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c328c29c7dd4273bec2451efdee281d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Token Count Comparison ---\n",
            "Note: DeepEval response token count is an estimation based on the reason field.\n",
            "\n",
            "DeepEval Evaluation Total Tokens:\n",
            "  Prompt Tokens: 2059\n",
            "  Estimated Response Tokens: 0\n",
            "  Grand Total Tokens: 2059\n",
            "\n",
            "Manual OpenAI API Call Total Tokens:\n",
            "  Prompt Tokens: 2059\n",
            "  Response Tokens: 438\n",
            "  Grand Total Tokens: 2497\n",
            "\n",
            "Observation: The DeepEval evaluation appears to use fewer tokens overall (based on this estimation).\n"
          ]
        }
      ],
      "source": [
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Assuming the CSV is already created and contains the data\n",
        "try:\n",
        "    df = pd.read_csv('ai_agent_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # --- DeepEval Evaluation with Token Counting ---\n",
        "\n",
        "    # Define the evaluation criteria and steps (same as before)\n",
        "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "    EVALUATION_STEPS = [\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
        "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
        "    ]\n",
        "    EVALUATION_PARAMS = [\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
        "    ]\n",
        "\n",
        "    # Create test cases\n",
        "    test_cases = []\n",
        "    for index, row in df.iterrows():\n",
        "        test_cases.append(\n",
        "            LLMTestCase(\n",
        "                input=row['Query'],\n",
        "                actual_output=row['Actual Response'],\n",
        "                expected_output=row['Expected Response']\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Create an EvaluationDataset and add test cases\n",
        "    dataset = EvaluationDataset()\n",
        "    for test_case in test_cases:\n",
        "        dataset.add_test_case(test_case)\n",
        "\n",
        "    # Instantiate the LoggingGEvalTokenCount (assuming it's defined in a previous cell)\n",
        "    try:\n",
        "        correctness_metric_deepeval = LoggingGEvalTokenCount(\n",
        "            name=\"Correctness\",\n",
        "            criteria=CRITERIA,\n",
        "            evaluation_steps=EVALUATION_STEPS,\n",
        "            evaluation_params=EVALUATION_PARAMS,\n",
        "            model=\"gpt-4o\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Running DeepEval Evaluation ---\")\n",
        "        for test_case in test_cases:\n",
        "            correctness_metric_deepeval.measure(test_case)\n",
        "\n",
        "        deepeval_total_prompt_tokens, deepeval_total_response_tokens = correctness_metric_deepeval.get_total_token_counts()\n",
        "        deepeval_grand_total_tokens = deepeval_total_prompt_tokens + deepeval_total_response_tokens\n",
        "\n",
        "    except NameError:\n",
        "        print(\"\\nError: LoggingGEvalTokenCount class not found. Please ensure the cell defining LoggingGEvalTokenCount is executed.\")\n",
        "        deepeval_total_prompt_tokens, deepeval_total_response_tokens, deepeval_grand_total_tokens = None, None, None\n",
        "\n",
        "    # --- Calculate Total Token Count for Manual OpenAI API Calls ---\n",
        "    manual_prompt_tokens_total = None\n",
        "    manual_response_tokens_total = None\n",
        "    manual_total_tokens = None\n",
        "\n",
        "    try:\n",
        "        # Assuming results_df from manual OpenAI calls is available from a previous cell\n",
        "        if 'results_df' in locals() and not results_df.empty:\n",
        "            manual_prompt_tokens_total = results_df['Prompt Tokens'].sum()\n",
        "            manual_response_tokens_total = results_df['Response Tokens'].sum()\n",
        "            manual_total_tokens = manual_prompt_tokens_total + manual_response_tokens_total\n",
        "        else:\n",
        "            print(\"\\nCould not find results_df from manual OpenAI calls to calculate totals.\")\n",
        "            print(\"Please ensure the cell with manual OpenAI evaluation was executed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError calculating manual token totals: {e}\")\n",
        "\n",
        "    # --- Token Count Comparison Summary ---\n",
        "    print(\"\\n--- Token Count Comparison ---\")\n",
        "    print(\"Note: DeepEval response token count is an estimation based on the reason field.\")\n",
        "\n",
        "    print(\"\\nDeepEval Evaluation Total Tokens:\")\n",
        "    if deepeval_total_prompt_tokens is not None:\n",
        "        print(f\"  Prompt Tokens: {deepeval_total_prompt_tokens}\")\n",
        "    if deepeval_total_response_tokens is not None:\n",
        "        print(f\"  Estimated Response Tokens: {deepeval_total_response_tokens}\")\n",
        "    if deepeval_grand_total_tokens is not None:\n",
        "        print(f\"  Grand Total Tokens: {deepeval_grand_total_tokens}\")\n",
        "    else:\n",
        "        print(\"  Could not calculate DeepEval total tokens.\")\n",
        "\n",
        "    print(\"\\nManual OpenAI API Call Total Tokens:\")\n",
        "    if manual_prompt_tokens_total is not None:\n",
        "        print(f\"  Prompt Tokens: {manual_prompt_tokens_total}\")\n",
        "    if manual_response_tokens_total is not None:\n",
        "        print(f\"  Response Tokens: {manual_response_tokens_total}\")\n",
        "    if manual_total_tokens is not None:\n",
        "        print(f\"  Grand Total Tokens: {manual_total_tokens}\")\n",
        "    else:\n",
        "        print(\"  Could not calculate manual total tokens.\")\n",
        "\n",
        "    # Optional: Add a simple comparison statement\n",
        "    if deepeval_grand_total_tokens is not None and manual_total_tokens is not None:\n",
        "        if manual_total_tokens < deepeval_grand_total_tokens:\n",
        "            print(\"\\nObservation: The manual OpenAI API calls appear to use fewer tokens overall (based on this estimation).\")\n",
        "        elif manual_total_tokens > deepeval_grand_total_tokens:\n",
        "            print(\"\\nObservation: The DeepEval evaluation appears to use fewer tokens overall (based on this estimation).\")\n",
        "        else:\n",
        "            print(\"\\nObservation: Both methods used a similar number of tokens (based on this estimation).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "0Shvx6qrd3tY",
        "outputId": "1e6c8297-bcdb-4ca8-952a-5f3cb15b8f50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Solve this math problem: 5x + 2 = 17\",\n          \"Tell me about the history of the internet.\",\n          \"How does photosynthesis work?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"5x = 15, so x = 3.\",\n          \"The internet originated from ARPANET in the late 1960s.\",\n          \"Photosynthesis is how plants convert light energy into chemical energy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Expected Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To solve 5x + 2 = 17, subtract 2 from both sides to get 5x = 15, then divide by 5 to find x = 3.\",\n          \"The internet's history began with ARPANET in the 1960s, evolving into the global network we use today.\",\n          \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll pigment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21730674684008833,\n        \"min\": 0.3,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evaluation Reason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Could not parse reason.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prompt Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 181,\n        \"max\": 239,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          227\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 32,\n        \"max\": 50,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4d423672-8dcb-4d07-97df-ae99fc596530\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Actual Response</th>\n",
              "      <th>Expected Response</th>\n",
              "      <th>Evaluation Score</th>\n",
              "      <th>Evaluation Reason</th>\n",
              "      <th>Prompt Tokens</th>\n",
              "      <th>Response Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris is the capital of France.</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>182</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about the history of the internet.</td>\n",
              "      <td>The internet originated from ARPANET in the la...</td>\n",
              "      <td>The internet's history began with ARPANET in t...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>210</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Write a short story about a robot learning to ...</td>\n",
              "      <td>Unit 734 processed data. One day, it saw a sun...</td>\n",
              "      <td>A heartwarming story about a robot named Unit ...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>208</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain quantum computing in simple terms.</td>\n",
              "      <td>Quantum computing uses quantum mechanics to so...</td>\n",
              "      <td>Quantum computing is a new type of computing t...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>204</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the benefits of exercise?</td>\n",
              "      <td>Exercise improves physical and mental health, ...</td>\n",
              "      <td>Regular exercise has numerous benefits, includ...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>203</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does photosynthesis work?</td>\n",
              "      <td>Photosynthesis is how plants convert light ene...</td>\n",
              "      <td>Photosynthesis is the process by which green p...</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>206</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recommend a good book to read.</td>\n",
              "      <td>I recommend 'To Kill a Mockingbird'.</td>\n",
              "      <td>A highly-rated and engaging book recommendatio...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>199</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the difference between a simile and a ...</td>\n",
              "      <td>A simile uses 'like' or 'as' to compare, a met...</td>\n",
              "      <td>A simile is a figure of speech that directly c...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>239</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Solve this math problem: 5x + 2 = 17</td>\n",
              "      <td>5x = 15, so x = 3.</td>\n",
              "      <td>To solve 5x + 2 = 17, subtract 2 from both sid...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>227</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Translate 'hello' into Spanish.</td>\n",
              "      <td>Hola.</td>\n",
              "      <td>The Spanish translation for 'hello' is 'Hola'.</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Could not parse reason.</td>\n",
              "      <td>181</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d423672-8dcb-4d07-97df-ae99fc596530')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d423672-8dcb-4d07-97df-ae99fc596530 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d423672-8dcb-4d07-97df-ae99fc596530');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-75871c08-91cf-4b33-bae5-a18ceb9ef2d8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75871c08-91cf-4b33-bae5-a18ceb9ef2d8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-75871c08-91cf-4b33-bae5-a18ceb9ef2d8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4161efc8-f9aa-4070-9746-9ce25ebf37b9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4161efc8-f9aa-4070-9746-9ce25ebf37b9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Query  \\\n",
              "0                     What is the capital of France?   \n",
              "1         Tell me about the history of the internet.   \n",
              "2  Write a short story about a robot learning to ...   \n",
              "3         Explain quantum computing in simple terms.   \n",
              "4                 What are the benefits of exercise?   \n",
              "5                      How does photosynthesis work?   \n",
              "6                     Recommend a good book to read.   \n",
              "7  What is the difference between a simile and a ...   \n",
              "8               Solve this math problem: 5x + 2 = 17   \n",
              "9                    Translate 'hello' into Spanish.   \n",
              "\n",
              "                                     Actual Response  \\\n",
              "0                    Paris is the capital of France.   \n",
              "1  The internet originated from ARPANET in the la...   \n",
              "2  Unit 734 processed data. One day, it saw a sun...   \n",
              "3  Quantum computing uses quantum mechanics to so...   \n",
              "4  Exercise improves physical and mental health, ...   \n",
              "5  Photosynthesis is how plants convert light ene...   \n",
              "6               I recommend 'To Kill a Mockingbird'.   \n",
              "7  A simile uses 'like' or 'as' to compare, a met...   \n",
              "8                                 5x = 15, so x = 3.   \n",
              "9                                              Hola.   \n",
              "\n",
              "                                   Expected Response  Evaluation Score  \\\n",
              "0                    The capital of France is Paris.               1.0   \n",
              "1  The internet's history began with ARPANET in t...               0.5   \n",
              "2  A heartwarming story about a robot named Unit ...               0.6   \n",
              "3  Quantum computing is a new type of computing t...               0.5   \n",
              "4  Regular exercise has numerous benefits, includ...               0.5   \n",
              "5  Photosynthesis is the process by which green p...               0.3   \n",
              "6  A highly-rated and engaging book recommendatio...               0.6   \n",
              "7  A simile is a figure of speech that directly c...               0.8   \n",
              "8  To solve 5x + 2 = 17, subtract 2 from both sid...               0.8   \n",
              "9     The Spanish translation for 'hello' is 'Hola'.               0.9   \n",
              "\n",
              "         Evaluation Reason  Prompt Tokens  Response Tokens  \n",
              "0  Could not parse reason.            182               36  \n",
              "1  Could not parse reason.            210               42  \n",
              "2  Could not parse reason.            208               50  \n",
              "3  Could not parse reason.            204               44  \n",
              "4  Could not parse reason.            203               50  \n",
              "5  Could not parse reason.            206               47  \n",
              "6  Could not parse reason.            199               32  \n",
              "7  Could not parse reason.            239               40  \n",
              "8  Could not parse reason.            227               47  \n",
              "9  Could not parse reason.            181               50  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW_giXK8emr9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3afGarRhpWK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Assuming the OPENAI_API_KEY is already set in the environment or Colab secrets\n",
        "client = OpenAI()\n",
        "\n",
        "# Load the data (assuming the CSV is already created)\n",
        "try:\n",
        "    df = pd.read_csv('ai_agent_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Define the evaluation criteria and steps\n",
        "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
        "    EVALUATION_STEPS = [\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
        "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
        "    ]\n",
        "\n",
        "    # Manually construct a single prompt for all test cases\n",
        "    steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(EVALUATION_STEPS))\n",
        "    full_prompt = f\"\"\"# Overall Evaluation Criteria\n",
        "{CRITERIA}\n",
        "\n",
        "# Overall Evaluation Steps\n",
        "{steps}\n",
        "\n",
        "# Data to Evaluate (Query, Actual Output, Expected Output pairs)\n",
        "Evaluate each pair below based on the criteria and steps provided above. For each pair, provide a brief justification (under 50 words) and a numerical score between 0.00 and 1.00 (2 decimal places).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # Add each data pair to the prompt\n",
        "    for index, row in df.iterrows():\n",
        "        full_prompt += f\"\"\"---\n",
        "Pair {index + 1}:\n",
        "Input: {row['Query']}\n",
        "Actual Output: {row['Actual Response']}\n",
        "Expected Output: {row['Expected Response']}\n",
        "\n",
        "Evaluation for Pair {index + 1}:\n",
        "\"\"\"\n",
        "\n",
        "    print(\"Constructed a single prompt for all queries:\")\n",
        "    print(full_prompt)\n",
        "    print(\"--- End of Full Prompt ---\")\n",
        "\n",
        "    model_name = \"gpt-4o\" # Define the model name here\n",
        "\n",
        "    # Calculate prompt token count\n",
        "    full_prompt_token_count = None\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model_name)\n",
        "        full_prompt_token_count = len(encoding.encode(full_prompt))\n",
        "        print(f\"\\n📊 Full Prompt token count ({model_name}): {full_prompt_token_count}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠️ Could not count tokens for full prompt ({model_name}): {e}\\n\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI API with the single prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI assistant that evaluates text based on provided criteria for multiple cases.\"},\n",
        "                {\"role\": \"user\", \"content\": full_prompt}\n",
        "            ],\n",
        "            max_tokens=1000 # Adjust max_tokens to accommodate evaluation for all pairs\n",
        "        )\n",
        "\n",
        "        evaluation_text = response.choices[0].message.content.strip()\n",
        "        print(\"OpenAI Response for all queries:\")\n",
        "        print(evaluation_text)\n",
        "        print(\"--- End of Full Response ---\")\n",
        "\n",
        "        # Calculate response token count\n",
        "        full_response_token_count = None\n",
        "        try:\n",
        "            encoding = tiktoken.encoding_for_model(model_name)\n",
        "            full_response_token_count = len(encoding.encode(evaluation_text))\n",
        "            print(f\"📊 Full Response token count ({model_name}): {full_response_token_count}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not count tokens for full response ({model_name}): {e}\\n\")\n",
        "\n",
        "        # Note: Parsing scores and reasons from a single response for multiple pairs\n",
        "        # would require more sophisticated parsing logic. This code focuses on token count.\n",
        "\n",
        "        # Display total token counts for this single-prompt approach\n",
        "        print(\"\\n--- Single-Prompt OpenAI API Call Token Totals ---\")\n",
        "        print(f\"Total Prompt Tokens (Single Call): {full_prompt_token_count}\")\n",
        "        print(f\"Total Response Tokens (Single Call): {full_response_token_count}\")\n",
        "        print(f\"Grand Total Tokens (Single Call): {full_prompt_token_count + full_response_token_count if full_prompt_token_count is not None and full_response_token_count is not None else None}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during single OpenAI API call: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goJtaaJniX7K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0368783e12e14d4e8c9bd3a74dccac65": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_96e89fcbae0a4af49a3e36172822962c",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠦</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠦\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "10f2b44d516142a3958a7c325cf1b045": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d01d6cb457c4afcaed345baa34b51dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fbcb62603e247678903875bf247c81a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206ecd48986d4a36b8341f0ed9a7c390": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1d01d6cb457c4afcaed345baa34b51dc",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠸</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠸\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "250701185ca545a591c10aaacc406d06": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2d0acb06eca94ae0b50b249ce2783f4b",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠋</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠋\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "29a26fb30d5b4bfc82f8a229aa3ac21e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29fe17bfee354e1da22ec40641c9ea84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0acb06eca94ae0b50b249ce2783f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e35d392d38b4fa897bf6fef46b77718": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34de899663cf4d7a9a19a634446f22fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2b27ba8a7144df91983c9210f8b291": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4310f9784c944188acf0a6d7d49bfd13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e66aab2b2cf4b738961ab900f0e1ddd": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_879d8b7ad9f04894a4dbbaf311221358",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "8267394bc077437c8b4d6b01bd9f23c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879d8b7ad9f04894a4dbbaf311221358": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ea57dd044e4d9b8e61efc4fdec741d": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4310f9784c944188acf0a6d7d49bfd13",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠧</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠧\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "96e89fcbae0a4af49a3e36172822962c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "994b914d680e4959ad9c15297ef71106": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_29fe17bfee354e1da22ec40641c9ea84",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "9c328c29c7dd4273bec2451efdee281d": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1fbcb62603e247678903875bf247c81a",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "b4f90540ee944cb98826ef512854f57a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8267394bc077437c8b4d6b01bd9f23c8",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠦</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠦\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "b917c1b93e7e46e8a674b177a542b464": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3d2b27ba8a7144df91983c9210f8b291",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠦</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠦\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "bbd69cd8ee2d4f62aa019ff8c2e88afd": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_29a26fb30d5b4bfc82f8a229aa3ac21e",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠦</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠦\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "cc31eb0261664e6a89e94f021452626d": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_34de899663cf4d7a9a19a634446f22fe",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "cf94f9ea705a474c9b777fa9cacec852": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2e35d392d38b4fa897bf6fef46b77718",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠇</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠇\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "f388e68f379c47b89915ea5d3aa5e513": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_10f2b44d516142a3958a7c325cf1b045",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠹</span>  ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness [GEval] Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
                  "text/plain": "\u001b[38;2;106;0;255m⠹\u001b[0m  ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness [GEval] Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
