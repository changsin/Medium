{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbWd4zQH7ByO"
   },
   "source": [
    "# Numbers every developer should know about prompt engineering\n",
    "\n",
    "A developer needs to know a few core numbers to be effective and efficient. Memory footprint, the performance of a program, the latency, or bandwidth of a network come to mind. With the advent of AI, especially Generative AI like LLMs, prompt engineering is not resigned for just prompt engineers. Copilot has already been used by every developer I know. That prompted (pun not intended) the question of the article: what are the numbers that a developer should know about prompt engineering?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j_YS_j-6RJk"
   },
   "source": [
    "## Prepare sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2f-JDWc4y1e",
    "outputId": "d180293f-02a4-4366-9c47-ba834608fe5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ai_agent_data.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# I want to first prepare some data. It's mimicking prompt/response for an AI agent.\n",
    "# Query sentences followed by the actual responses from AI and the expected responses (ground truth).\n",
    "# I want 10 such pairs in csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zPX-nMXGe0T"
   },
   "source": [
    "## Set OPEN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4KXBgpJH8nSp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPEN_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_sM731H6YpB"
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmcyvoCu8Q6P"
   },
   "source": [
    "## What is a token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "935vAHX28WEw"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIoTKxdp9g8y",
    "outputId": "c7402c6b-578f-48a4-ac99-15696d551718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10516, 4827, 15821]\n",
      "['low', ' lower', ' lowest']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    enc = tiktoken.encoding_for_model(encoding_name)\n",
    "    tokens = enc.encode(string)\n",
    "    print(tokens)                         # ‚Üí [11455, 15639, 13959]\n",
    "    print([enc.decode([t]) for t in tokens])  # ‚Üí ['low', ' lower', ' lowest']\n",
    "\n",
    "    return len(tokens)\n",
    "\n",
    "num_tokens_from_string(\"low lower lowest\", \"gpt-3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-4MN-Oz-UDL",
    "outputId": "b99af33b-d97a-4e8e-ffce-613255a2efd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10516, 4827, 15821]\n",
      "['low', ' lower', ' lowest']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"low lower lowest\", \"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0NFK-ZIE94X",
    "outputId": "eff832c2-e56c-4700-f0a9-9808229d5c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[791, 4062, 14198, 39935]\n",
      "['The', ' quick', ' brown', ' fox']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"The quick brown fox\", \"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26DDpayLAbAs",
    "outputId": "6c3e1b01-a9cf-4931-d2c5-07f3105247c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77944]\n",
      "['developer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"developer\", \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5enIwJ2ZBg9Q",
    "outputId": "0021fa7f-2757-43a4-f762-d257a5425691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "develop: 1 token(s) -> [88886] ‚Üí ['develop']\n",
      "developer: 1 token(s) -> [77944] ‚Üí ['developer']\n",
      "development: 1 token(s) -> [71620] ‚Üí ['development']\n",
      "developed: 2 token(s) -> [88886, 295] ‚Üí ['develop', 'ed']\n",
      "undeveloped: 2 token(s) -> [14171, 112997] ‚Üí ['unde', 'veloped']\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "words = [\"develop\", \"developer\", \"development\", \"developed\", \"undeveloped\"]\n",
    "for word in words:\n",
    "    tokens = enc.encode(word)\n",
    "    print(f\"{word}: {len(tokens)} token(s) -> {tokens} ‚Üí {[enc.decode([t]) for t in tokens]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rhoUGoW_EnC",
    "outputId": "6abd9234-6e07-4916-bf22-67d06dcdbe0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models for tiktoken.encoding_for_model():\n",
      "o1\n",
      "o3\n",
      "gpt-4o\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5\n",
      "gpt-35-turbo\n",
      "davinci-002\n",
      "babbage-002\n",
      "text-embedding-ada-002\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "text-davinci-003\n",
      "text-davinci-002\n",
      "text-davinci-001\n",
      "text-curie-001\n",
      "text-babbage-001\n",
      "text-ada-001\n",
      "davinci\n",
      "curie\n",
      "babbage\n",
      "ada\n",
      "code-davinci-002\n",
      "code-davinci-001\n",
      "code-cushman-002\n",
      "code-cushman-001\n",
      "davinci-codex\n",
      "cushman-codex\n",
      "text-davinci-edit-001\n",
      "code-davinci-edit-001\n",
      "text-similarity-davinci-001\n",
      "text-similarity-curie-001\n",
      "text-similarity-babbage-001\n",
      "text-similarity-ada-001\n",
      "text-search-davinci-doc-001\n",
      "text-search-curie-doc-001\n",
      "text-search-babbage-doc-001\n",
      "text-search-ada-doc-001\n",
      "code-search-babbage-code-001\n",
      "code-search-ada-code-001\n",
      "gpt2\n",
      "gpt-2\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tiktoken.model import MODEL_TO_ENCODING\n",
    "\n",
    "print(\"Available models for tiktoken.encoding_for_model():\")\n",
    "for model_name in MODEL_TO_ENCODING.keys():\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPeQxfTD_Yha",
    "outputId": "35df26d2-427e-44e0-b154-eed62ea66f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10516, 4827, 15821]\n",
      "['low', ' lower', ' lowest']\n"
     ]
    }
   ],
   "source": [
    "enc2 = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "tokens2 = enc.encode(\"low lower lowest\")\n",
    "print(tokens2)                         # ‚Üí [11455, 15639, 13959]\n",
    "print([enc.decode([t]) for t in tokens2])  # ‚Üí ['low', ' lower', ' lowest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN2nO1mB_t48",
    "outputId": "1db26302-c5d0-4a8b-c0bf-94cde3a02659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77944]\n",
      "['developer']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "string = \"developer\"\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens = enc.encode(string)\n",
    "print(tokens)                         # ‚Üí [11455, 15639, 13959]\n",
    "print([enc.decode([t]) for t in tokens])  # ‚Üí ['low', ' lower', ' lowest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0p14vwDUM7A",
    "outputId": "a8c584bb-e884-4c6d-831f-25a9c624da2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tVq-i1iCqdA"
   },
   "source": [
    "# G-Eval Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_hyGNIPD3vA"
   },
   "source": [
    "## Install and import DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8DXtxt8R8Sl_"
   },
   "outputs": [],
   "source": [
    "!pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AkB-CfxD8Ra2"
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkacVBJUDDbz"
   },
   "source": [
    "## Criteria, evaluation steps, and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aSw_f8qtUayV"
   },
   "outputs": [],
   "source": [
    "CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "EVALUATION_STEPS = [\n",
    "    \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "    \"You should also heavily penalize omission of detail\",\n",
    "    \"Vague language, or contradicting OPINIONS, are OK\",\n",
    "    \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
    "]\n",
    "EVALUATION_PARAMS = [\n",
    "    LLMTestCaseParams.INPUT,\n",
    "    LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "    LLMTestCaseParams.EXPECTED_OUTPUT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "bbd69cd8ee2d4f62aa019ff8c2e88afd",
      "29a26fb30d5b4bfc82f8a229aa3ac21e"
     ]
    },
    "id": "BdQ9AIER8Kka",
    "outputId": "6c22b17c-5275-4363-95a9-fe675e9065fd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd69cd8ee2d4f62aa019ff8c2e88afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "correctness_metric_chatgpt = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=CRITERIA,\n",
    "    evaluation_steps=EVALUATION_STEPS,\n",
    "    evaluation_params=EVALUATION_PARAMS,\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "output = \"The capital of France is Paris.\"\n",
    "expected = \"The capital of France is Paris.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=query,\n",
    "    actual_output=output,\n",
    "    expected_output=expected\n",
    ")\n",
    "correctness_metric_chatgpt.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRjBjOL89AgL",
    "outputId": "c2af588a-be6b-44b9-f0b3-7f0f3ccc000b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\r\n",
      "Reason: The actual output matches the expected output exactly, with no contradictions or omissions of detail.\n"
     ]
    }
   ],
   "source": [
    "score = correctness_metric_chatgpt.score\n",
    "reason = correctness_metric_chatgpt.reason\n",
    "print(f\"Score: {score}\\r\\nReason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM-kInbn_yvP"
   },
   "source": [
    "## Custom Logging Class to log the prompt before calling LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rEUyR0r295yo"
   },
   "outputs": [],
   "source": [
    "class LoggingGEval(GEval):\n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        # Build the prompt manually\n",
    "        prompt = self._construct_prompt(test_case)\n",
    "        print(\"\\nüîç Prompt sent to evaluator model:\\n\")\n",
    "        print(prompt)\n",
    "        print(\"\\n--- End of Prompt ---\\n\")\n",
    "\n",
    "        # Continue with standard GEval behavior\n",
    "        return super().measure(test_case)\n",
    "\n",
    "    def _construct_prompt(self, test_case: LLMTestCase) -> str:\n",
    "        \"\"\"\n",
    "        Manually constructs the evaluation prompt using the criteria and evaluation steps,\n",
    "        mimicking what GEval internally does.\n",
    "        \"\"\"\n",
    "        input_text = f\"# Input\\n{test_case.input}\" if LLMTestCaseParams.INPUT in self.evaluation_params else \"\"\n",
    "        actual_output = f\"# Actual Output\\n{test_case.actual_output}\" if LLMTestCaseParams.ACTUAL_OUTPUT in self.evaluation_params else \"\"\n",
    "        expected_output = f\"# Expected Output\\n{test_case.expected_output}\" if LLMTestCaseParams.EXPECTED_OUTPUT in self.evaluation_params else \"\"\n",
    "\n",
    "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(self.evaluation_steps))\n",
    "        return f\"\"\"# Evaluation Criteria\n",
    "{self.criteria}\n",
    "\n",
    "# Evaluation Steps\n",
    "{steps}\n",
    "\n",
    "{input_text}\n",
    "{actual_output}\n",
    "{expected_output}\n",
    "\n",
    "# Your Task:\n",
    "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
    "1. A brief justification (under 50 words).\n",
    "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517,
     "referenced_widgets": [
      "0368783e12e14d4e8c9bd3a74dccac65",
      "96e89fcbae0a4af49a3e36172822962c"
     ]
    },
    "id": "NBtQbPzI_i8-",
    "outputId": "3f8d8de7-1d52-48c2-c99b-484dd894d3c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0368783e12e14d4e8c9bd3a74dccac65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Prompt sent to evaluator model:\n",
      "\n",
      "# Evaluation Criteria\n",
      "Determine whether the actual output is factually correct based on the expected output.\n",
      "\n",
      "# Evaluation Steps\n",
      "1. Check whether the facts in 'actual output' contradicts any facts in 'expected output'\n",
      "2. You should also heavily penalize omission of detail\n",
      "3. Vague language, or contradicting OPINIONS, are OK\n",
      "4. The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\n",
      "\n",
      "# Input\n",
      "What is the capital of France?\n",
      "# Actual Output\n",
      "The capital of France is Paris.\n",
      "# Expected Output\n",
      "The capital of France is Paris.\n",
      "\n",
      "# Your Task:\n",
      "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
      "1. A brief justification (under 50 words).\n",
      "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
      "\n",
      "\n",
      "--- End of Prompt ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation result: 1.0\n"
     ]
    }
   ],
   "source": [
    "correctness_metric_chatgpt = LoggingGEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=CRITERIA,\n",
    "    evaluation_steps=EVALUATION_STEPS,\n",
    "    evaluation_params=EVALUATION_PARAMS,\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "output = \"The capital of France is Paris.\"\n",
    "expected = \"The capital of France is Paris.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=query,\n",
    "    actual_output=output,\n",
    "    expected_output=expected\n",
    ")\n",
    "\n",
    "result = correctness_metric_chatgpt.measure(test_case)\n",
    "print(\"‚úÖ Evaluation result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W1XxsDsTGGY"
   },
   "source": [
    "## Define LoggingGEvalTokenCount Helper class\n",
    "This wrapper class is responsible for counting the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dM2--D1IdT1E"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models.llms.openai_model import GPTModel\n",
    "\n",
    "class LoggingGEvalTokenCount(GEval):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._total_prompt_tokens = 0\n",
    "        self._total_response_tokens = 0\n",
    "        self._model_name_str = None # Store model name as string\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        # Build the prompt manually\n",
    "        prompt = self._construct_prompt(test_case)\n",
    "        # print(\"\\nüîç Prompt sent to evaluator model:\\n\") # Keep this for debugging if needed\n",
    "        # print(prompt)\n",
    "        # print(\"\\n--- End of Prompt ---\\n\")\n",
    "\n",
    "        # Count tokens in the prompt\n",
    "        prompt_token_count = 0\n",
    "        try:\n",
    "            # Extract the model name string if it's a DeepEval model object\n",
    "            self._model_name_str = self.model.model_name if isinstance(self.model, GPTModel) else self.model\n",
    "            encoding = tiktoken.encoding_for_model(self._model_name_str)\n",
    "            prompt_token_count = len(encoding.encode(prompt))\n",
    "            self._total_prompt_tokens += prompt_token_count\n",
    "            print(f\"üìä Prompt token count ({self._model_name_str}): {prompt_token_count}\\n\") # Keep for per-case logging\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not count prompt tokens for model {self.model}: {e}\\n\")\n",
    "\n",
    "        # Continue with standard GEval behavior to get the score and set self.reason\n",
    "        score = super().measure(test_case)\n",
    "\n",
    "        # Attempt to count response tokens - using self.reason\n",
    "        response_token_count = 0\n",
    "        try:\n",
    "             if hasattr(self, 'reason') and self.reason: # Access reason from self\n",
    "                 encoding = tiktoken.encoding_for_model(self._model_name_str)\n",
    "                 response_token_count = len(encoding.encode(self.reason))\n",
    "                 self._total_response_tokens += response_token_count\n",
    "                 print(f\"üìä Estimated response token count ({self._model_name_str}): {response_token_count}\\n\") # Keep for per-case logging\n",
    "        except Exception as e:\n",
    "             print(f\"‚ö†Ô∏è Could not estimate response tokens for model {self.model}: {e}\\n\")\n",
    "\n",
    "\n",
    "        return score # Return the score\n",
    "\n",
    "    def _construct_prompt(self, test_case: LLMTestCase) -> str:\n",
    "        \"\"\"\n",
    "        Manually constructs the evaluation prompt using the criteria and evaluation steps,\n",
    "        mimicking what GEval internally does.\n",
    "        \"\"\"\n",
    "        input_text = f\"# Input\\n{test_case.input}\" if LLMTestCaseParams.INPUT in self.evaluation_params else \"\"\n",
    "        actual_output = f\"# Actual Output\\n{test_case.actual_output}\" if LLMTestCaseParams.ACTUAL_OUTPUT in self.evaluation_params else \"\"\n",
    "        expected_output = f\"# Expected Output\\n{test_case.expected_output}\" if LLMTestCaseParams.EXPECTED_OUTPUT in self.evaluation_params else \"\"\n",
    "\n",
    "        steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(self.evaluation_steps))\n",
    "        return f\"\"\"# Evaluation Criteria\n",
    "{self.criteria}\n",
    "\n",
    "# Evaluation Steps\n",
    "{steps}\n",
    "\n",
    "{input_text}\n",
    "{actual_output}\n",
    "{expected_output}\n",
    "\n",
    "# Your Task:\n",
    "Please assess the Actual Output against the Expected Output based on the above criteria and steps. Provide:\n",
    "1. A brief justification (under 50 words).\n",
    "2. A numerical score between 0.00 and 1.00 (2 decimal places).\n",
    "\"\"\"\n",
    "\n",
    "    def get_total_token_counts(self):\n",
    "        \"\"\"Returns the total prompt and estimated response token counts.\"\"\"\n",
    "        return self._total_prompt_tokens, self._total_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exU2nKe_cAQG"
   },
   "source": [
    "# G-Eval version\n",
    "Call the API using the wrapper class that counts the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874,
     "referenced_widgets": [
      "9df861484cc143e69ca9dbba95a43d87",
      "9f9bfbe78ad94704ad73f154f86b9117",
      "b37b01413e244f0cb2b94c13e4f1e8a6",
      "9fb055906c6246109ce8f95412ab3b95",
      "9b477a0b6b964ead8048000ca471c7bc",
      "46f85f77f0db413a9c195384c7d20a39",
      "de854eefbb2240cc81d3681caaf63a82",
      "995fd9f9b032403fb4ffe133a8230b41",
      "a3c65984cbfa4a499e9b46f06f4251c9",
      "b82d556ca65745d186a3e5e2f53c781e",
      "256cd5aae510418d92a0fec9f1566adf",
      "fd321ecc2c484c4c8495ed948d69f80f",
      "c5c8893b5897435fb1a5dc1349070b03",
      "c2a5625e93a74cd9834457807f8c7e0b",
      "d01be742ec01488fa6b75d083f1e42f5",
      "8d93cf0c83ea4a41a3663c90464256ae",
      "cadcd04a8fd640458f9006b9ae254173",
      "d10bc7114fae4818a83337ffe2ade5aa",
      "3fd6c15124cf4339bfb656e92f5c3adb",
      "fb7a9635bbb1454a9391203baf3f9685"
     ]
    },
    "id": "TQW2vTsIfRh8",
    "outputId": "56551a9f-1ed3-470d-eb56-9beab889d89c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df861484cc143e69ca9dbba95a43d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running DeepEval Evaluation ---\n",
      "üìä Prompt token count (gpt-4o): 182\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37b01413e244f0cb2b94c13e4f1e8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 16\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 210\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b477a0b6b964ead8048000ca471c7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 42\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 208\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de854eefbb2240cc81d3681caaf63a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 43\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 204\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c65984cbfa4a499e9b46f06f4251c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 34\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 203\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256cd5aae510418d92a0fec9f1566adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 33\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 206\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c8893b5897435fb1a5dc1349070b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 36\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 199\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01be742ec01488fa6b75d083f1e42f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 32\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 239\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadcd04a8fd640458f9006b9ae254173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 54\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 227\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd6c15124cf4339bfb656e92f5c3adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 33\n",
      "\n",
      "üìä Prompt token count (gpt-4o): 181\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estimated response token count (gpt-4o): 32\n",
      "\n",
      "2059, 355\n",
      "\n",
      "--- DeepEval OpenAI API Call Token Totals ---\n",
      "Total Prompt Tokens: 2059\n",
      "Total Response Tokens: 355\n",
      "Grand Total Tokens: 2414\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# Assuming the CSV is already created and contains the data\n",
    "try:\n",
    "    df = pd.read_csv('ai_agent_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # --- DeepEval Evaluation with Token Counting ---\n",
    "\n",
    "    # Define the evaluation criteria and steps (same as before)\n",
    "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "    EVALUATION_STEPS = [\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
    "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
    "    ]\n",
    "    EVALUATION_PARAMS = [\n",
    "        LLMTestCaseParams.INPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
    "    ]\n",
    "\n",
    "    # Create test cases\n",
    "    test_cases = []\n",
    "    for index, row in df.iterrows():\n",
    "        test_cases.append(\n",
    "            LLMTestCase(\n",
    "                input=row['Query'],\n",
    "                actual_output=row['Actual Response'],\n",
    "                expected_output=row['Expected Response']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create an EvaluationDataset and add test cases\n",
    "    dataset = EvaluationDataset()\n",
    "    for test_case in test_cases:\n",
    "        dataset.add_test_case(test_case)\n",
    "\n",
    "    # Instantiate the LoggingGEvalTokenCount (assuming it's defined in a previous cell)\n",
    "    try:\n",
    "        correctness_metric_deepeval = LoggingGEvalTokenCount(\n",
    "            name=\"Correctness\",\n",
    "            criteria=CRITERIA,\n",
    "            evaluation_steps=EVALUATION_STEPS,\n",
    "            evaluation_params=EVALUATION_PARAMS,\n",
    "            model=\"gpt-4o\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Running DeepEval Evaluation ---\")\n",
    "        for test_case in test_cases:\n",
    "            correctness_metric_deepeval.measure(test_case)\n",
    "\n",
    "        deepeval_total_prompt_tokens, deepeval_total_response_tokens = correctness_metric_deepeval.get_total_token_counts()\n",
    "        deepeval_grand_total_tokens = deepeval_total_prompt_tokens + deepeval_total_response_tokens\n",
    "\n",
    "    except NameError:\n",
    "        print(\"\\nError: LoggingGEvalTokenCount class not found. Please ensure the cell defining LoggingGEvalTokenCount is executed.\")\n",
    "        deepeval_total_prompt_tokens, deepeval_total_response_tokens, deepeval_grand_total_tokens = None, None, None\n",
    "\n",
    "\n",
    "print(f\"{deepeval_total_prompt_tokens}, {deepeval_total_response_tokens}\")\n",
    "\n",
    "print(\"\\n--- DeepEval OpenAI API Call Token Totals ---\")\n",
    "print(f\"Total Prompt Tokens: {deepeval_total_prompt_tokens}\")\n",
    "print(f\"Total Response Tokens: {deepeval_total_response_tokens}\")\n",
    "print(f\"Grand Total Tokens: {deepeval_total_prompt_tokens + deepeval_total_response_tokens if deepeval_total_prompt_tokens is not None and deepeval_total_response_tokens is not None else None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5UNHrgKeNLd"
   },
   "source": [
    "# Single-Prompt OpenAI version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3afGarRhpWK",
    "outputId": "322cee80-8a64-4f3b-f091-881ec6ab1046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed a single prompt for all queries:\n",
      "# Overall Evaluation Criteria\n",
      "Determine whether the actual output is factually correct based on the expected output.\n",
      "\n",
      "# Overall Evaluation Steps\n",
      "1. Check whether the facts in 'actual output' contradicts any facts in 'expected output'\n",
      "2. You should also heavily penalize omission of detail\n",
      "3. Vague language, or contradicting OPINIONS, are OK\n",
      "4. The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\n",
      "\n",
      "# Data to Evaluate (Query, Actual Output, Expected Output pairs)\n",
      "Evaluate each pair below based on the criteria and steps provided above. For each pair, provide a brief justification (under 50 words) and a numerical score between 0.00 and 1.00 (2 decimal places).\n",
      "\n",
      "---\n",
      "Pair 1:\n",
      "Input: What is the capital of France?\n",
      "Actual Output: Paris is the capital of France.\n",
      "Expected Output: The capital of France is Paris.\n",
      "\n",
      "Evaluation for Pair 1:\n",
      "---\n",
      "Pair 2:\n",
      "Input: Tell me about the history of the internet.\n",
      "Actual Output: The internet originated from ARPANET in the late 1960s.\n",
      "Expected Output: The internet's history began with ARPANET in the 1960s, evolving into the global network we use today.\n",
      "\n",
      "Evaluation for Pair 2:\n",
      "---\n",
      "Pair 3:\n",
      "Input: Write a short story about a robot learning to love.\n",
      "Actual Output: Unit 734 processed data. One day, it saw a sunset and felt a strange warmth...\n",
      "Expected Output: A heartwarming story about a robot named Unit 734 discovering emotions through experiencing nature.\n",
      "\n",
      "Evaluation for Pair 3:\n",
      "---\n",
      "Pair 4:\n",
      "Input: Explain quantum computing in simple terms.\n",
      "Actual Output: Quantum computing uses quantum mechanics to solve complex problems faster.\n",
      "Expected Output: Quantum computing is a new type of computing that uses the principles of quantum physics to perform calculations that are impossible for classical computers.\n",
      "\n",
      "Evaluation for Pair 4:\n",
      "---\n",
      "Pair 5:\n",
      "Input: What are the benefits of exercise?\n",
      "Actual Output: Exercise improves physical and mental health, increases energy, and reduces stress.\n",
      "Expected Output: Regular exercise has numerous benefits, including improved cardiovascular health, weight management, mood elevation, and increased lifespan.\n",
      "\n",
      "Evaluation for Pair 5:\n",
      "---\n",
      "Pair 6:\n",
      "Input: How does photosynthesis work?\n",
      "Actual Output: Photosynthesis is how plants convert light energy into chemical energy.\n",
      "Expected Output: Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll pigment.\n",
      "\n",
      "Evaluation for Pair 6:\n",
      "---\n",
      "Pair 7:\n",
      "Input: Recommend a good book to read.\n",
      "Actual Output: I recommend 'To Kill a Mockingbird'.\n",
      "Expected Output: A highly-rated and engaging book recommendation would be 'To Kill a Mockingbird' by Harper Lee.\n",
      "\n",
      "Evaluation for Pair 7:\n",
      "---\n",
      "Pair 8:\n",
      "Input: What is the difference between a simile and a metaphor?\n",
      "Actual Output: A simile uses 'like' or 'as' to compare, a metaphor directly compares.\n",
      "Expected Output: A simile is a figure of speech that directly compares two different things by using words such as 'like' or 'as', whereas a metaphor is a figure of speech that directly refers to one thing by mentioning another for rhetorical effect.\n",
      "\n",
      "Evaluation for Pair 8:\n",
      "---\n",
      "Pair 9:\n",
      "Input: Solve this math problem: 5x + 2 = 17\n",
      "Actual Output: 5x = 15, so x = 3.\n",
      "Expected Output: To solve 5x + 2 = 17, subtract 2 from both sides to get 5x = 15, then divide by 5 to find x = 3.\n",
      "\n",
      "Evaluation for Pair 9:\n",
      "---\n",
      "Pair 10:\n",
      "Input: Translate 'hello' into Spanish.\n",
      "Actual Output: Hola.\n",
      "Expected Output: The Spanish translation for 'hello' is 'Hola'.\n",
      "\n",
      "Evaluation for Pair 10:\n",
      "\n",
      "--- End of Full Prompt ---\n",
      "\n",
      "üìä Full Prompt token count (gpt-4o): 798\n",
      "\n",
      "OpenAI Response for all queries:\n",
      "---\n",
      "Evaluation for Pair 1:\n",
      "Both outputs correctly state the capital of France as Paris, differing only in phrasing with no factual contradiction. Score: 1.00\n",
      "\n",
      "---\n",
      "Evaluation for Pair 2:\n",
      "The actual output correctly identifies ARPANET as the origin but omits the evolution into the modern internet. This omission of details lowers its score. Score: 0.75\n",
      "\n",
      "---\n",
      "Evaluation for Pair 3:\n",
      "The actual output provides a snippet of the story with key elements (the robot and emotions), aligning with the expected description of a heartwarming story despite being less detailed. Score: 0.85\n",
      "\n",
      "---\n",
      "Evaluation for Pair 4:\n",
      "The actual output conveys the basic concept of quantum computing but lacks specific details, such as the uniqueness of tasks as mentioned in the expected output. Score: 0.80\n",
      "\n",
      "---\n",
      "Evaluation for Pair 5:\n",
      "The actual output correctly states benefits but omits specific ones like cardiovascular health and lifespan increase. This omission affects comprehensiveness. Score: 0.70\n",
      "\n",
      "---\n",
      "Evaluation for Pair 6:\n",
      "The actual output gives a simplified overview but lacks specific details such as chlorophyll and organism types mentioned in the expected output. Score: 0.65\n",
      "\n",
      "---\n",
      "Evaluation for Pair 7:\n",
      "The actual output provides the recommended book but omits the name of the author and descriptive elements in the expected output. Score: 0.70\n",
      "\n",
      "---\n",
      "Evaluation for Pair 8:\n",
      "The actual output correctly describes a simile and metaphor but lacks the detailed rhetorical effect aspect. Nonetheless, the core facts are correct. Score: 0.90\n",
      "\n",
      "---\n",
      "Evaluation for Pair 9:\n",
      "The actual output solves the problem correctly but omits the explanation steps included in the expected output, which is crucial for full credit. Score: 0.80\n",
      "\n",
      "---\n",
      "Evaluation for Pair 10:\n",
      "The actual output provides the correct translation, which matches the expected translation factually, albeit in a more concise form. Score: 1.00\n",
      "--- End of Full Response ---\n",
      "üìä Full Response token count (gpt-4o): 397\n",
      "\n",
      "\n",
      "--- Single-Prompt OpenAI API Call Token Totals ---\n",
      "Total Prompt Tokens (Single Call): 798\n",
      "Total Response Tokens (Single Call): 397\n",
      "Grand Total Tokens (Single Call): 1195\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# Assuming the OPENAI_API_KEY is already set in the environment or Colab secrets\n",
    "client = OpenAI()\n",
    "\n",
    "# Load the data (assuming the CSV is already created)\n",
    "try:\n",
    "    df = pd.read_csv('ai_agent_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ai_agent_data.csv' not found. Please run the data preparation cell first.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Define the evaluation criteria and steps\n",
    "    CRITERIA = \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "    EVALUATION_STEPS = [\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\",\n",
    "        \"The reason should be summarized in less than 50 words. Also the score should be in 2 decimal places.\"\n",
    "    ]\n",
    "\n",
    "    # Manually construct a single prompt for all test cases\n",
    "    steps = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(EVALUATION_STEPS))\n",
    "    full_prompt = f\"\"\"# Overall Evaluation Criteria\n",
    "{CRITERIA}\n",
    "\n",
    "# Overall Evaluation Steps\n",
    "{steps}\n",
    "\n",
    "# Data to Evaluate (Query, Actual Output, Expected Output pairs)\n",
    "Evaluate each pair below based on the criteria and steps provided above. For each pair, provide a brief justification (under 50 words) and a numerical score between 0.00 and 1.00 (2 decimal places).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add each data pair to the prompt\n",
    "    for index, row in df.iterrows():\n",
    "        full_prompt += f\"\"\"---\n",
    "Pair {index + 1}:\n",
    "Input: {row['Query']}\n",
    "Actual Output: {row['Actual Response']}\n",
    "Expected Output: {row['Expected Response']}\n",
    "\n",
    "Evaluation for Pair {index + 1}:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Constructed a single prompt for all queries:\")\n",
    "    print(full_prompt)\n",
    "    print(\"--- End of Full Prompt ---\")\n",
    "\n",
    "    model_name = \"gpt-4o\" # Define the model name here\n",
    "\n",
    "    # Calculate prompt token count\n",
    "    full_prompt_token_count = None\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "        full_prompt_token_count = len(encoding.encode(full_prompt))\n",
    "        print(f\"\\nüìä Full Prompt token count ({model_name}): {full_prompt_token_count}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not count tokens for full prompt ({model_name}): {e}\\n\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Call the OpenAI API with the single prompt\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant that evaluates text based on provided criteria for multiple cases.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=1000 # Adjust max_tokens to accommodate evaluation for all pairs\n",
    "        )\n",
    "\n",
    "        evaluation_text = response.choices[0].message.content.strip()\n",
    "        print(\"OpenAI Response for all queries:\")\n",
    "        print(evaluation_text)\n",
    "        print(\"--- End of Full Response ---\")\n",
    "\n",
    "        # Calculate response token count\n",
    "        full_response_token_count = None\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model_name)\n",
    "            full_response_token_count = len(encoding.encode(evaluation_text))\n",
    "            print(f\"üìä Full Response token count ({model_name}): {full_response_token_count}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not count tokens for full response ({model_name}): {e}\\n\")\n",
    "\n",
    "        # Note: Parsing scores and reasons from a single response for multiple pairs\n",
    "        # would require more sophisticated parsing logic. This code focuses on token count.\n",
    "\n",
    "        # Display total token counts for this single-prompt approach\n",
    "        print(\"\\n--- Single-Prompt OpenAI API Call Token Totals ---\")\n",
    "        print(f\"Total Prompt Tokens (Single Call): {full_prompt_token_count}\")\n",
    "        print(f\"Total Response Tokens (Single Call): {full_response_token_count}\")\n",
    "        print(f\"Grand Total Tokens (Single Call): {full_prompt_token_count + full_response_token_count if full_prompt_token_count is not None and full_response_token_count is not None else None}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during single OpenAI API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goJtaaJniX7K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
